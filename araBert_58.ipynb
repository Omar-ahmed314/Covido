{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTdhD3kWthvg",
        "outputId": "1d223bc5-9b5c-4041-bf1d-d12ef087c9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.8/dist-packages (0.0.14)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farasapy) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.8/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabert in /usr/local/lib/python3.8/dist-packages (1.0.1)\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.8/dist-packages (from arabert) (0.0.14)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.8/dist-packages (from arabert) (1.4.2)\n",
            "Requirement already satisfied: PyArabic in /usr/local/lib/python3.8/dist-packages (from arabert) (0.6.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert) (4.64.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from PyArabic->arabert) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert) (3.0.4)\n",
            "fatal: destination path 'arabert' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!pip install arabert\n",
        "!git clone https://github.com/aub-mind/arabert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T_XUVZViLLr",
        "outputId": "dcd1eb69-5cc6-47f8-d539-6ce623f355b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "00tImzUPcdAQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O3gA8JUWGPw",
        "outputId": "9d844aa3-5f64-4f83-f6f4-758ff8571f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "RrfhcS-cWGTH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8KTjc7-lrC4c"
      },
      "outputs": [],
      "source": [
        "model_name = \"aubmindlab/bert-base-arabertv02-twitter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Ocy6RcQQbpwY"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"train.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "_TmsWIbdrC4f"
      },
      "outputs": [],
      "source": [
        "arabert_prep = ArabertPreprocessor(model_name=model_name, keep_emojis=True)\n",
        "df_train['text']=df_train['text'].apply(arabert_prep.preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "5-qZJtQXrC4h"
      },
      "outputs": [],
      "source": [
        "possible_labels = df_train.stance.unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(possible_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nfb4J0yDXRi",
        "outputId": "181d19b1-ebf0-45ee-f319-eea8175e341b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  0 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "FfRa2NcErC4j"
      },
      "outputs": [],
      "source": [
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "JRnHapMvrC4l"
      },
      "outputs": [],
      "source": [
        "df_train.stance = df_train['stance'].map(label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4s1n77_B9vK",
        "outputId": "2501e783-2abf-4e99-af8b-52b455bf1f7f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 0, 0: 1, -1: 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "HF1Y4rwtc7UR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db0cceb-8dd2-4b83-9c6d-5235bef3e577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6988\n",
            "6988\n"
          ]
        }
      ],
      "source": [
        "y_train=df_train.stance.values\n",
        "print(len(y_train))\n",
        "x_train=df_train.text.values\n",
        "print(len(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "30hKqG6_bpyv"
      },
      "outputs": [],
      "source": [
        "df_val = pd.read_csv(\"dev.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "B7ZI1BsOrC4n"
      },
      "outputs": [],
      "source": [
        "arabert_prep = ArabertPreprocessor(model_name=model_name, keep_emojis=True)\n",
        "df_val['text']=df_val['text'].apply(arabert_prep.preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1h7jqt3RrC4p"
      },
      "outputs": [],
      "source": [
        "df_val.stance = df_val['stance'].map(label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "qpr6gqOJdKau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c981b70-7b34-4dbd-ebc8-0cc7758d387b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "y_val=df_val.stance.values\n",
        "print(len(y_val))\n",
        "x_val=df_val.text.values\n",
        "print(len(x_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxtfcoo0fqOk"
      },
      "source": [
        "# BERT CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "rFA0bbFfeVd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d3ce40-00ea-448f-f372-0f794224994c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 50 µs, sys: 0 ns, total: 50 µs\n",
            "Wall time: 54.4 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        # D_in = 256 if version == \"mini\" else 768\n",
        "        # H, D_out = 50, 2\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv02-twitter\")\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        #----------- classifier ---------\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768,50),    #768 bert output => linear input\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(50, 3) # no of classes\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "          # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
        "                              \n",
        "          \n",
        "          # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "          # Feed input to classifier to compute logits\n",
        "          # feed el hidden layer embedding to the classifier layer\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qkYqh_rjCyR"
      },
      "source": [
        "# get input IDs and attention masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "yaC23SBDjP5_"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "VtYaJAnOjGeN"
      },
      "outputs": [],
      "source": [
        "def getIDs_attention(data):\n",
        "  # Create empty lists to store outputs\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  #tokenizer = AutoTokenizer.from_pretrained(model_name) if version == \"mini\" else AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "\n",
        "  # For every sentence...\n",
        "  for i,sent in enumerate(data):\n",
        "      # `encode_plus` will:\n",
        "      #    (1) Tokenize the sentence\n",
        "      #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "      #    (3) Truncate/Pad sentence to max length\n",
        "      #    (4) Map tokens to their IDs\n",
        "      #    (5) Create attention mask\n",
        "      #    (6) Return a dictionary of outputs\n",
        "      encoded_sent = tokenizer.encode_plus(\n",
        "          text=sent,  # Preprocess sentence\n",
        "          add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "          max_length=64,                  # Max length to truncate/pad\n",
        "          padding='max_length',        # Pad sentence to max length\n",
        "          #return_tensors='pt',           # Return PyTorch tensor\n",
        "          return_attention_mask=True,     # Return attention mask\n",
        "          truncation = True \n",
        "          )\n",
        "      \n",
        "      # Add the outputs to the lists\n",
        "      input_ids.append(encoded_sent.get('input_ids'))\n",
        "      attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "    # Convert lists to tensors\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "  return input_ids, attention_masks\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "7_er5ccMk8Dy"
      },
      "outputs": [],
      "source": [
        "# get ids and mask attentions for train and val data to feed them to the model\n",
        "train_inputs, train_masks = getIDs_attention(x_train)\n",
        "val_inputs, val_masks = getIDs_attention(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "cCJU9la9qeUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f53eda-2c1c-4510-af34-d25e1eee894f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6988\n"
          ]
        }
      ],
      "source": [
        "print(len(train_masks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Lbrlme8otxFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7fd9fa-fb84-4abe-dc80-c0e387c6f99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6988\n",
            "6988\n",
            "6988\n",
            "6988\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "print(len(train_data))\n",
        "print(len(train_inputs))\n",
        "print(len(train_masks))\n",
        "print(len(train_labels))\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sldCHNGLn03M"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "QdLWTC-cnxLH"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.optim import SparseAdam, Adam\n",
        "def initialize_model(epochs=4, version=\"mini\"):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=True)\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(params=list(bert_classifier.parameters()),\n",
        "                      lr=0.001,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    #Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer ,scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "6sAt0AZKnxND"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, optimizer,scheduler,train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "             \n",
        "           \n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "            # Zero out any previously calculated gradients\n",
        "            \n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "      \n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5VEy7SOoXG6"
      },
      "source": [
        "# evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "oqcclEMCnxQf"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv7jR_kqogkx"
      },
      "source": [
        "# Initialize and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "bqLTHlKVuBMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bf6115-0f27-4de4-c4b8-b258efdbf602"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.739869   |     -      |     -     |   1.34   \n",
            "   1    |   40    |   0.582240   |     -      |     -     |   1.26   \n",
            "   1    |   60    |   0.567578   |     -      |     -     |   1.26   \n",
            "   1    |   80    |   0.490732   |     -      |     -     |   1.29   \n",
            "   1    |   100   |   0.529322   |     -      |     -     |   1.28   \n",
            "   1    |   120   |   0.454137   |     -      |     -     |   1.29   \n",
            "   1    |   140   |   0.492276   |     -      |     -     |   1.30   \n",
            "   1    |   160   |   0.542691   |     -      |     -     |   1.31   \n",
            "   1    |   180   |   0.536304   |     -      |     -     |   1.31   \n",
            "   1    |   200   |   0.538270   |     -      |     -     |   1.32   \n",
            "   1    |   220   |   0.509966   |     -      |     -     |   1.32   \n",
            "   1    |   240   |   0.426080   |     -      |     -     |   1.32   \n",
            "   1    |   260   |   0.467258   |     -      |     -     |   1.32   \n",
            "   1    |   280   |   0.465475   |     -      |     -     |   1.31   \n",
            "   1    |   300   |   0.496979   |     -      |     -     |   1.30   \n",
            "   1    |   320   |   0.480406   |     -      |     -     |   1.29   \n",
            "   1    |   340   |   0.472881   |     -      |     -     |   1.28   \n",
            "   1    |   360   |   0.416529   |     -      |     -     |   1.28   \n",
            "   1    |   380   |   0.508695   |     -      |     -     |   1.26   \n",
            "   1    |   400   |   0.464834   |     -      |     -     |   1.24   \n",
            "   1    |   420   |   0.504804   |     -      |     -     |   1.24   \n",
            "   1    |   436   |   0.488041   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.508684   |  0.444772  |   81.55   |   31.88  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.502891   |     -      |     -     |   1.29   \n",
            "   2    |   40    |   0.498531   |     -      |     -     |   1.22   \n",
            "   2    |   60    |   0.516844   |     -      |     -     |   1.22   \n",
            "   2    |   80    |   0.437751   |     -      |     -     |   1.22   \n",
            "   2    |   100   |   0.468275   |     -      |     -     |   1.22   \n",
            "   2    |   120   |   0.445122   |     -      |     -     |   1.22   \n",
            "   2    |   140   |   0.436462   |     -      |     -     |   1.22   \n",
            "   2    |   160   |   0.508507   |     -      |     -     |   1.22   \n",
            "   2    |   180   |   0.495445   |     -      |     -     |   1.22   \n",
            "   2    |   200   |   0.491740   |     -      |     -     |   1.22   \n",
            "   2    |   220   |   0.477668   |     -      |     -     |   1.22   \n",
            "   2    |   240   |   0.397976   |     -      |     -     |   1.22   \n",
            "   2    |   260   |   0.435280   |     -      |     -     |   1.22   \n",
            "   2    |   280   |   0.490726   |     -      |     -     |   1.22   \n",
            "   2    |   300   |   0.520696   |     -      |     -     |   1.22   \n",
            "   2    |   320   |   0.463725   |     -      |     -     |   1.22   \n",
            "   2    |   340   |   0.456209   |     -      |     -     |   1.22   \n",
            "   2    |   360   |   0.381917   |     -      |     -     |   1.22   \n",
            "   2    |   380   |   0.489094   |     -      |     -     |   1.22   \n",
            "   2    |   400   |   0.401469   |     -      |     -     |   1.22   \n",
            "   2    |   420   |   0.523361   |     -      |     -     |   1.22   \n",
            "   2    |   436   |   0.406072   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.466347   |  0.458226  |   81.35   |   30.31  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.516504   |     -      |     -     |   1.27   \n",
            "   3    |   40    |   0.482676   |     -      |     -     |   1.22   \n",
            "   3    |   60    |   0.508648   |     -      |     -     |   1.22   \n",
            "   3    |   80    |   0.441477   |     -      |     -     |   1.22   \n",
            "   3    |   100   |   0.443926   |     -      |     -     |   1.22   \n",
            "   3    |   120   |   0.403354   |     -      |     -     |   1.22   \n",
            "   3    |   140   |   0.407448   |     -      |     -     |   1.22   \n",
            "   3    |   160   |   0.499208   |     -      |     -     |   1.23   \n",
            "   3    |   180   |   0.488859   |     -      |     -     |   1.22   \n",
            "   3    |   200   |   0.459165   |     -      |     -     |   1.22   \n",
            "   3    |   220   |   0.459019   |     -      |     -     |   1.22   \n",
            "   3    |   240   |   0.401397   |     -      |     -     |   1.22   \n",
            "   3    |   260   |   0.426113   |     -      |     -     |   1.22   \n",
            "   3    |   280   |   0.440957   |     -      |     -     |   1.22   \n",
            "   3    |   300   |   0.508514   |     -      |     -     |   1.22   \n",
            "   3    |   320   |   0.458075   |     -      |     -     |   1.22   \n",
            "   3    |   340   |   0.453834   |     -      |     -     |   1.22   \n",
            "   3    |   360   |   0.390133   |     -      |     -     |   1.22   \n",
            "   3    |   380   |   0.471082   |     -      |     -     |   1.22   \n",
            "   3    |   400   |   0.432662   |     -      |     -     |   1.22   \n",
            "   3    |   420   |   0.522386   |     -      |     -     |   1.22   \n",
            "   3    |   436   |   0.420737   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.456652   |  0.433570  |   81.65   |   30.45  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.487030   |     -      |     -     |   1.28   \n",
            "   4    |   40    |   0.464548   |     -      |     -     |   1.22   \n",
            "   4    |   60    |   0.531841   |     -      |     -     |   1.22   \n",
            "   4    |   80    |   0.442756   |     -      |     -     |   1.22   \n",
            "   4    |   100   |   0.434171   |     -      |     -     |   1.22   \n",
            "   4    |   120   |   0.413846   |     -      |     -     |   1.24   \n",
            "   4    |   140   |   0.410031   |     -      |     -     |   1.22   \n",
            "   4    |   160   |   0.486485   |     -      |     -     |   1.22   \n",
            "   4    |   180   |   0.484701   |     -      |     -     |   1.22   \n",
            "   4    |   200   |   0.457230   |     -      |     -     |   1.22   \n",
            "   4    |   220   |   0.426907   |     -      |     -     |   1.22   \n",
            "   4    |   240   |   0.423074   |     -      |     -     |   1.24   \n",
            "   4    |   260   |   0.408302   |     -      |     -     |   1.22   \n",
            "   4    |   280   |   0.465036   |     -      |     -     |   1.22   \n",
            "   4    |   300   |   0.462565   |     -      |     -     |   1.22   \n",
            "   4    |   320   |   0.430431   |     -      |     -     |   1.22   \n",
            "   4    |   340   |   0.440216   |     -      |     -     |   1.22   \n",
            "   4    |   360   |   0.358862   |     -      |     -     |   1.22   \n",
            "   4    |   380   |   0.481342   |     -      |     -     |   1.22   \n",
            "   4    |   400   |   0.387964   |     -      |     -     |   1.22   \n",
            "   4    |   420   |   0.510371   |     -      |     -     |   1.22   \n",
            "   4    |   436   |   0.405456   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.446518   |  0.429932  |   81.75   |   30.48  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.451568   |     -      |     -     |   1.29   \n",
            "   5    |   40    |   0.458836   |     -      |     -     |   1.22   \n",
            "   5    |   60    |   0.539863   |     -      |     -     |   1.22   \n",
            "   5    |   80    |   0.441434   |     -      |     -     |   1.22   \n",
            "   5    |   100   |   0.464918   |     -      |     -     |   1.22   \n",
            "   5    |   120   |   0.395002   |     -      |     -     |   1.22   \n",
            "   5    |   140   |   0.375547   |     -      |     -     |   1.22   \n",
            "   5    |   160   |   0.465240   |     -      |     -     |   1.22   \n",
            "   5    |   180   |   0.464009   |     -      |     -     |   1.22   \n",
            "   5    |   200   |   0.433003   |     -      |     -     |   1.22   \n",
            "   5    |   220   |   0.436668   |     -      |     -     |   1.22   \n",
            "   5    |   240   |   0.378544   |     -      |     -     |   1.22   \n",
            "   5    |   260   |   0.404786   |     -      |     -     |   1.22   \n",
            "   5    |   280   |   0.436916   |     -      |     -     |   1.22   \n",
            "   5    |   300   |   0.428398   |     -      |     -     |   1.22   \n",
            "   5    |   320   |   0.507514   |     -      |     -     |   1.22   \n",
            "   5    |   340   |   0.417145   |     -      |     -     |   1.22   \n",
            "   5    |   360   |   0.358032   |     -      |     -     |   1.22   \n",
            "   5    |   380   |   0.461614   |     -      |     -     |   1.22   \n",
            "   5    |   400   |   0.383254   |     -      |     -     |   1.22   \n",
            "   5    |   420   |   0.510365   |     -      |     -     |   1.22   \n",
            "   5    |   436   |   0.393373   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   5    |    -    |   0.437068   |  0.430297  |   82.04   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   6    |   20    |   0.459812   |     -      |     -     |   1.28   \n",
            "   6    |   40    |   0.448339   |     -      |     -     |   1.22   \n",
            "   6    |   60    |   0.493729   |     -      |     -     |   1.22   \n",
            "   6    |   80    |   0.408189   |     -      |     -     |   1.22   \n",
            "   6    |   100   |   0.429751   |     -      |     -     |   1.22   \n",
            "   6    |   120   |   0.415670   |     -      |     -     |   1.22   \n",
            "   6    |   140   |   0.367734   |     -      |     -     |   1.22   \n",
            "   6    |   160   |   0.466356   |     -      |     -     |   1.22   \n",
            "   6    |   180   |   0.440863   |     -      |     -     |   1.22   \n",
            "   6    |   200   |   0.463690   |     -      |     -     |   1.22   \n",
            "   6    |   220   |   0.417330   |     -      |     -     |   1.22   \n",
            "   6    |   240   |   0.372427   |     -      |     -     |   1.22   \n",
            "   6    |   260   |   0.387777   |     -      |     -     |   1.22   \n",
            "   6    |   280   |   0.434662   |     -      |     -     |   1.22   \n",
            "   6    |   300   |   0.454458   |     -      |     -     |   1.22   \n",
            "   6    |   320   |   0.436997   |     -      |     -     |   1.22   \n",
            "   6    |   340   |   0.406982   |     -      |     -     |   1.22   \n",
            "   6    |   360   |   0.345736   |     -      |     -     |   1.22   \n",
            "   6    |   380   |   0.478191   |     -      |     -     |   1.22   \n",
            "   6    |   400   |   0.386203   |     -      |     -     |   1.22   \n",
            "   6    |   420   |   0.490687   |     -      |     -     |   1.22   \n",
            "   6    |   436   |   0.378706   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   6    |    -    |   0.427073   |  0.433650  |   82.64   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   7    |   20    |   0.423430   |     -      |     -     |   1.29   \n",
            "   7    |   40    |   0.469210   |     -      |     -     |   1.22   \n",
            "   7    |   60    |   0.499626   |     -      |     -     |   1.22   \n",
            "   7    |   80    |   0.447301   |     -      |     -     |   1.22   \n",
            "   7    |   100   |   0.457578   |     -      |     -     |   1.22   \n",
            "   7    |   120   |   0.366251   |     -      |     -     |   1.21   \n",
            "   7    |   140   |   0.367972   |     -      |     -     |   1.22   \n",
            "   7    |   160   |   0.457177   |     -      |     -     |   1.22   \n",
            "   7    |   180   |   0.437464   |     -      |     -     |   1.22   \n",
            "   7    |   200   |   0.424677   |     -      |     -     |   1.22   \n",
            "   7    |   220   |   0.401045   |     -      |     -     |   1.22   \n",
            "   7    |   240   |   0.388283   |     -      |     -     |   1.22   \n",
            "   7    |   260   |   0.372387   |     -      |     -     |   1.23   \n",
            "   7    |   280   |   0.450596   |     -      |     -     |   1.22   \n",
            "   7    |   300   |   0.414172   |     -      |     -     |   1.22   \n",
            "   7    |   320   |   0.411599   |     -      |     -     |   1.22   \n",
            "   7    |   340   |   0.384194   |     -      |     -     |   1.22   \n",
            "   7    |   360   |   0.341932   |     -      |     -     |   1.22   \n",
            "   7    |   380   |   0.475751   |     -      |     -     |   1.22   \n",
            "   7    |   400   |   0.354337   |     -      |     -     |   1.22   \n",
            "   7    |   420   |   0.472559   |     -      |     -     |   1.22   \n",
            "   7    |   436   |   0.394944   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   7    |    -    |   0.418978   |  0.433716  |   82.34   |   30.46  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   8    |   20    |   0.438734   |     -      |     -     |   1.29   \n",
            "   8    |   40    |   0.433478   |     -      |     -     |   1.22   \n",
            "   8    |   60    |   0.530281   |     -      |     -     |   1.22   \n",
            "   8    |   80    |   0.458106   |     -      |     -     |   1.22   \n",
            "   8    |   100   |   0.412620   |     -      |     -     |   1.22   \n",
            "   8    |   120   |   0.394103   |     -      |     -     |   1.22   \n",
            "   8    |   140   |   0.352123   |     -      |     -     |   1.22   \n",
            "   8    |   160   |   0.471408   |     -      |     -     |   1.22   \n",
            "   8    |   180   |   0.431019   |     -      |     -     |   1.22   \n",
            "   8    |   200   |   0.453728   |     -      |     -     |   1.22   \n",
            "   8    |   220   |   0.395160   |     -      |     -     |   1.22   \n",
            "   8    |   240   |   0.353881   |     -      |     -     |   1.22   \n",
            "   8    |   260   |   0.399810   |     -      |     -     |   1.22   \n",
            "   8    |   280   |   0.428459   |     -      |     -     |   1.22   \n",
            "   8    |   300   |   0.434407   |     -      |     -     |   1.22   \n",
            "   8    |   320   |   0.433922   |     -      |     -     |   1.22   \n",
            "   8    |   340   |   0.440262   |     -      |     -     |   1.22   \n",
            "   8    |   360   |   0.336062   |     -      |     -     |   1.22   \n",
            "   8    |   380   |   0.412614   |     -      |     -     |   1.22   \n",
            "   8    |   400   |   0.359076   |     -      |     -     |   1.22   \n",
            "   8    |   420   |   0.480486   |     -      |     -     |   1.22   \n",
            "   8    |   436   |   0.405735   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   8    |    -    |   0.420882   |  0.430737  |   82.34   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   9    |   20    |   0.442689   |     -      |     -     |   1.29   \n",
            "   9    |   40    |   0.436304   |     -      |     -     |   1.22   \n",
            "   9    |   60    |   0.507140   |     -      |     -     |   1.22   \n",
            "   9    |   80    |   0.430435   |     -      |     -     |   1.22   \n",
            "   9    |   100   |   0.410206   |     -      |     -     |   1.22   \n",
            "   9    |   120   |   0.377966   |     -      |     -     |   1.22   \n",
            "   9    |   140   |   0.369095   |     -      |     -     |   1.22   \n",
            "   9    |   160   |   0.461514   |     -      |     -     |   1.22   \n",
            "   9    |   180   |   0.440404   |     -      |     -     |   1.22   \n",
            "   9    |   200   |   0.444119   |     -      |     -     |   1.22   \n",
            "   9    |   220   |   0.414215   |     -      |     -     |   1.22   \n",
            "   9    |   240   |   0.377414   |     -      |     -     |   1.22   \n",
            "   9    |   260   |   0.376838   |     -      |     -     |   1.22   \n",
            "   9    |   280   |   0.414243   |     -      |     -     |   1.22   \n",
            "   9    |   300   |   0.450397   |     -      |     -     |   1.22   \n",
            "   9    |   320   |   0.426379   |     -      |     -     |   1.22   \n",
            "   9    |   340   |   0.397587   |     -      |     -     |   1.22   \n",
            "   9    |   360   |   0.328414   |     -      |     -     |   1.22   \n",
            "   9    |   380   |   0.433154   |     -      |     -     |   1.22   \n",
            "   9    |   400   |   0.333857   |     -      |     -     |   1.22   \n",
            "   9    |   420   |   0.442392   |     -      |     -     |   1.22   \n",
            "   9    |   436   |   0.380980   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "   9    |    -    |   0.413807   |  0.436129  |   82.54   |   30.44  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  10    |   20    |   0.420256   |     -      |     -     |   1.28   \n",
            "  10    |   40    |   0.423818   |     -      |     -     |   1.22   \n",
            "  10    |   60    |   0.495319   |     -      |     -     |   1.22   \n",
            "  10    |   80    |   0.405092   |     -      |     -     |   1.22   \n",
            "  10    |   100   |   0.406589   |     -      |     -     |   1.22   \n",
            "  10    |   120   |   0.367685   |     -      |     -     |   1.22   \n",
            "  10    |   140   |   0.363109   |     -      |     -     |   1.22   \n",
            "  10    |   160   |   0.476849   |     -      |     -     |   1.22   \n",
            "  10    |   180   |   0.445677   |     -      |     -     |   1.22   \n",
            "  10    |   200   |   0.465951   |     -      |     -     |   1.22   \n",
            "  10    |   220   |   0.395766   |     -      |     -     |   1.22   \n",
            "  10    |   240   |   0.343748   |     -      |     -     |   1.22   \n",
            "  10    |   260   |   0.380105   |     -      |     -     |   1.22   \n",
            "  10    |   280   |   0.503777   |     -      |     -     |   1.22   \n",
            "  10    |   300   |   0.401345   |     -      |     -     |   1.22   \n",
            "  10    |   320   |   0.414197   |     -      |     -     |   1.22   \n",
            "  10    |   340   |   0.404954   |     -      |     -     |   1.22   \n",
            "  10    |   360   |   0.315356   |     -      |     -     |   1.22   \n",
            "  10    |   380   |   0.371726   |     -      |     -     |   1.22   \n",
            "  10    |   400   |   0.394164   |     -      |     -     |   1.22   \n",
            "  10    |   420   |   0.504234   |     -      |     -     |   1.22   \n",
            "  10    |   436   |   0.395246   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  10    |    -    |   0.413589   |  0.427396  |   82.54   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  11    |   20    |   0.410346   |     -      |     -     |   1.29   \n",
            "  11    |   40    |   0.430797   |     -      |     -     |   1.22   \n",
            "  11    |   60    |   0.445158   |     -      |     -     |   1.22   \n",
            "  11    |   80    |   0.380588   |     -      |     -     |   1.22   \n",
            "  11    |   100   |   0.426734   |     -      |     -     |   1.22   \n",
            "  11    |   120   |   0.357668   |     -      |     -     |   1.22   \n",
            "  11    |   140   |   0.341636   |     -      |     -     |   1.22   \n",
            "  11    |   160   |   0.494120   |     -      |     -     |   1.22   \n",
            "  11    |   180   |   0.428584   |     -      |     -     |   1.22   \n",
            "  11    |   200   |   0.421505   |     -      |     -     |   1.22   \n",
            "  11    |   220   |   0.411608   |     -      |     -     |   1.22   \n",
            "  11    |   240   |   0.358259   |     -      |     -     |   1.22   \n",
            "  11    |   260   |   0.373278   |     -      |     -     |   1.22   \n",
            "  11    |   280   |   0.464486   |     -      |     -     |   1.22   \n",
            "  11    |   300   |   0.443666   |     -      |     -     |   1.22   \n",
            "  11    |   320   |   0.412416   |     -      |     -     |   1.22   \n",
            "  11    |   340   |   0.409951   |     -      |     -     |   1.22   \n",
            "  11    |   360   |   0.337743   |     -      |     -     |   1.22   \n",
            "  11    |   380   |   0.400223   |     -      |     -     |   1.22   \n",
            "  11    |   400   |   0.364223   |     -      |     -     |   1.22   \n",
            "  11    |   420   |   0.465581   |     -      |     -     |   1.22   \n",
            "  11    |   436   |   0.407408   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  11    |    -    |   0.408467   |  0.431510  |   82.64   |   30.45  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  12    |   20    |   0.405891   |     -      |     -     |   1.29   \n",
            "  12    |   40    |   0.448024   |     -      |     -     |   1.22   \n",
            "  12    |   60    |   0.463727   |     -      |     -     |   1.22   \n",
            "  12    |   80    |   0.403673   |     -      |     -     |   1.22   \n",
            "  12    |   100   |   0.439796   |     -      |     -     |   1.22   \n",
            "  12    |   120   |   0.372002   |     -      |     -     |   1.22   \n",
            "  12    |   140   |   0.330926   |     -      |     -     |   1.22   \n",
            "  12    |   160   |   0.471795   |     -      |     -     |   1.22   \n",
            "  12    |   180   |   0.410992   |     -      |     -     |   1.22   \n",
            "  12    |   200   |   0.401864   |     -      |     -     |   1.22   \n",
            "  12    |   220   |   0.387572   |     -      |     -     |   1.22   \n",
            "  12    |   240   |   0.354623   |     -      |     -     |   1.22   \n",
            "  12    |   260   |   0.333872   |     -      |     -     |   1.22   \n",
            "  12    |   280   |   0.484985   |     -      |     -     |   1.22   \n",
            "  12    |   300   |   0.431636   |     -      |     -     |   1.22   \n",
            "  12    |   320   |   0.412658   |     -      |     -     |   1.22   \n",
            "  12    |   340   |   0.384597   |     -      |     -     |   1.22   \n",
            "  12    |   360   |   0.332958   |     -      |     -     |   1.22   \n",
            "  12    |   380   |   0.410941   |     -      |     -     |   1.22   \n",
            "  12    |   400   |   0.382872   |     -      |     -     |   1.22   \n",
            "  12    |   420   |   0.458972   |     -      |     -     |   1.22   \n",
            "  12    |   436   |   0.359947   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  12    |    -    |   0.404239   |  0.436762  |   82.54   |   30.47  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  13    |   20    |   0.389102   |     -      |     -     |   1.27   \n",
            "  13    |   40    |   0.411122   |     -      |     -     |   1.22   \n",
            "  13    |   60    |   0.455514   |     -      |     -     |   1.22   \n",
            "  13    |   80    |   0.402405   |     -      |     -     |   1.22   \n",
            "  13    |   100   |   0.409158   |     -      |     -     |   1.22   \n",
            "  13    |   120   |   0.363938   |     -      |     -     |   1.21   \n",
            "  13    |   140   |   0.353349   |     -      |     -     |   1.22   \n",
            "  13    |   160   |   0.472661   |     -      |     -     |   1.22   \n",
            "  13    |   180   |   0.423981   |     -      |     -     |   1.22   \n",
            "  13    |   200   |   0.405564   |     -      |     -     |   1.22   \n",
            "  13    |   220   |   0.379128   |     -      |     -     |   1.22   \n",
            "  13    |   240   |   0.358591   |     -      |     -     |   1.22   \n",
            "  13    |   260   |   0.371864   |     -      |     -     |   1.22   \n",
            "  13    |   280   |   0.421931   |     -      |     -     |   1.22   \n",
            "  13    |   300   |   0.428970   |     -      |     -     |   1.22   \n",
            "  13    |   320   |   0.416317   |     -      |     -     |   1.22   \n",
            "  13    |   340   |   0.386066   |     -      |     -     |   1.23   \n",
            "  13    |   360   |   0.362809   |     -      |     -     |   1.22   \n",
            "  13    |   380   |   0.389472   |     -      |     -     |   1.22   \n",
            "  13    |   400   |   0.365521   |     -      |     -     |   1.22   \n",
            "  13    |   420   |   0.504695   |     -      |     -     |   1.22   \n",
            "  13    |   436   |   0.368389   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  13    |    -    |   0.402120   |  0.442052  |   82.64   |   30.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  14    |   20    |   0.445004   |     -      |     -     |   1.28   \n",
            "  14    |   40    |   0.430516   |     -      |     -     |   1.22   \n",
            "  14    |   60    |   0.489112   |     -      |     -     |   1.22   \n",
            "  14    |   80    |   0.414873   |     -      |     -     |   1.22   \n",
            "  14    |   100   |   0.387823   |     -      |     -     |   1.22   \n",
            "  14    |   120   |   0.355789   |     -      |     -     |   1.22   \n",
            "  14    |   140   |   0.332416   |     -      |     -     |   1.22   \n",
            "  14    |   160   |   0.449670   |     -      |     -     |   1.22   \n",
            "  14    |   180   |   0.405283   |     -      |     -     |   1.22   \n",
            "  14    |   200   |   0.436048   |     -      |     -     |   1.22   \n",
            "  14    |   220   |   0.424477   |     -      |     -     |   1.22   \n",
            "  14    |   240   |   0.369287   |     -      |     -     |   1.22   \n",
            "  14    |   260   |   0.386453   |     -      |     -     |   1.22   \n",
            "  14    |   280   |   0.429639   |     -      |     -     |   1.22   \n",
            "  14    |   300   |   0.421377   |     -      |     -     |   1.22   \n",
            "  14    |   320   |   0.423040   |     -      |     -     |   1.22   \n",
            "  14    |   340   |   0.360430   |     -      |     -     |   1.22   \n",
            "  14    |   360   |   0.326651   |     -      |     -     |   1.22   \n",
            "  14    |   380   |   0.412243   |     -      |     -     |   1.22   \n",
            "  14    |   400   |   0.326305   |     -      |     -     |   1.22   \n",
            "  14    |   420   |   0.445209   |     -      |     -     |   1.22   \n",
            "  14    |   436   |   0.351940   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  14    |    -    |   0.401622   |  0.460154  |   82.84   |   30.44  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  15    |   20    |   0.434611   |     -      |     -     |   1.27   \n",
            "  15    |   40    |   0.391518   |     -      |     -     |   1.22   \n",
            "  15    |   60    |   0.442779   |     -      |     -     |   1.22   \n",
            "  15    |   80    |   0.382864   |     -      |     -     |   1.22   \n",
            "  15    |   100   |   0.396013   |     -      |     -     |   1.22   \n",
            "  15    |   120   |   0.349815   |     -      |     -     |   1.22   \n",
            "  15    |   140   |   0.358822   |     -      |     -     |   1.22   \n",
            "  15    |   160   |   0.448110   |     -      |     -     |   1.22   \n",
            "  15    |   180   |   0.406082   |     -      |     -     |   1.22   \n",
            "  15    |   200   |   0.450553   |     -      |     -     |   1.22   \n",
            "  15    |   220   |   0.426349   |     -      |     -     |   1.22   \n",
            "  15    |   240   |   0.333679   |     -      |     -     |   1.22   \n",
            "  15    |   260   |   0.371438   |     -      |     -     |   1.22   \n",
            "  15    |   280   |   0.432441   |     -      |     -     |   1.22   \n",
            "  15    |   300   |   0.431894   |     -      |     -     |   1.22   \n",
            "  15    |   320   |   0.407693   |     -      |     -     |   1.22   \n",
            "  15    |   340   |   0.395301   |     -      |     -     |   1.22   \n",
            "  15    |   360   |   0.296793   |     -      |     -     |   1.22   \n",
            "  15    |   380   |   0.408334   |     -      |     -     |   1.22   \n",
            "  15    |   400   |   0.355096   |     -      |     -     |   1.22   \n",
            "  15    |   420   |   0.482748   |     -      |     -     |   1.22   \n",
            "  15    |   436   |   0.366294   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  15    |    -    |   0.398979   |  0.449725  |   83.23   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  16    |   20    |   0.379070   |     -      |     -     |   1.27   \n",
            "  16    |   40    |   0.419496   |     -      |     -     |   1.22   \n",
            "  16    |   60    |   0.492480   |     -      |     -     |   1.22   \n",
            "  16    |   80    |   0.386857   |     -      |     -     |   1.22   \n",
            "  16    |   100   |   0.422807   |     -      |     -     |   1.22   \n",
            "  16    |   120   |   0.381322   |     -      |     -     |   1.22   \n",
            "  16    |   140   |   0.335090   |     -      |     -     |   1.22   \n",
            "  16    |   160   |   0.445790   |     -      |     -     |   1.22   \n",
            "  16    |   180   |   0.414964   |     -      |     -     |   1.22   \n",
            "  16    |   200   |   0.433832   |     -      |     -     |   1.22   \n",
            "  16    |   220   |   0.415042   |     -      |     -     |   1.22   \n",
            "  16    |   240   |   0.350158   |     -      |     -     |   1.22   \n",
            "  16    |   260   |   0.337633   |     -      |     -     |   1.22   \n",
            "  16    |   280   |   0.424006   |     -      |     -     |   1.22   \n",
            "  16    |   300   |   0.414556   |     -      |     -     |   1.22   \n",
            "  16    |   320   |   0.406970   |     -      |     -     |   1.22   \n",
            "  16    |   340   |   0.377616   |     -      |     -     |   1.22   \n",
            "  16    |   360   |   0.328294   |     -      |     -     |   1.22   \n",
            "  16    |   380   |   0.363348   |     -      |     -     |   1.22   \n",
            "  16    |   400   |   0.320724   |     -      |     -     |   1.22   \n",
            "  16    |   420   |   0.456924   |     -      |     -     |   1.22   \n",
            "  16    |   436   |   0.346900   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  16    |    -    |   0.393751   |  0.432185  |   82.94   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  17    |   20    |   0.376156   |     -      |     -     |   1.29   \n",
            "  17    |   40    |   0.449708   |     -      |     -     |   1.21   \n",
            "  17    |   60    |   0.461080   |     -      |     -     |   1.22   \n",
            "  17    |   80    |   0.406302   |     -      |     -     |   1.22   \n",
            "  17    |   100   |   0.388257   |     -      |     -     |   1.22   \n",
            "  17    |   120   |   0.340509   |     -      |     -     |   1.22   \n",
            "  17    |   140   |   0.327809   |     -      |     -     |   1.22   \n",
            "  17    |   160   |   0.455532   |     -      |     -     |   1.22   \n",
            "  17    |   180   |   0.442975   |     -      |     -     |   1.22   \n",
            "  17    |   200   |   0.391214   |     -      |     -     |   1.22   \n",
            "  17    |   220   |   0.372749   |     -      |     -     |   1.22   \n",
            "  17    |   240   |   0.330611   |     -      |     -     |   1.22   \n",
            "  17    |   260   |   0.371910   |     -      |     -     |   1.22   \n",
            "  17    |   280   |   0.444827   |     -      |     -     |   1.22   \n",
            "  17    |   300   |   0.386299   |     -      |     -     |   1.22   \n",
            "  17    |   320   |   0.447256   |     -      |     -     |   1.22   \n",
            "  17    |   340   |   0.407084   |     -      |     -     |   1.22   \n",
            "  17    |   360   |   0.307242   |     -      |     -     |   1.22   \n",
            "  17    |   380   |   0.397029   |     -      |     -     |   1.22   \n",
            "  17    |   400   |   0.350660   |     -      |     -     |   1.22   \n",
            "  17    |   420   |   0.432420   |     -      |     -     |   1.22   \n",
            "  17    |   436   |   0.377004   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  17    |    -    |   0.393961   |  0.443414  |   83.04   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  18    |   20    |   0.404269   |     -      |     -     |   1.28   \n",
            "  18    |   40    |   0.415115   |     -      |     -     |   1.22   \n",
            "  18    |   60    |   0.480180   |     -      |     -     |   1.22   \n",
            "  18    |   80    |   0.347946   |     -      |     -     |   1.22   \n",
            "  18    |   100   |   0.376420   |     -      |     -     |   1.22   \n",
            "  18    |   120   |   0.357135   |     -      |     -     |   1.22   \n",
            "  18    |   140   |   0.329795   |     -      |     -     |   1.22   \n",
            "  18    |   160   |   0.495568   |     -      |     -     |   1.22   \n",
            "  18    |   180   |   0.412954   |     -      |     -     |   1.22   \n",
            "  18    |   200   |   0.392886   |     -      |     -     |   1.22   \n",
            "  18    |   220   |   0.373420   |     -      |     -     |   1.22   \n",
            "  18    |   240   |   0.332422   |     -      |     -     |   1.22   \n",
            "  18    |   260   |   0.401289   |     -      |     -     |   1.22   \n",
            "  18    |   280   |   0.407721   |     -      |     -     |   1.22   \n",
            "  18    |   300   |   0.399004   |     -      |     -     |   1.22   \n",
            "  18    |   320   |   0.401710   |     -      |     -     |   1.22   \n",
            "  18    |   340   |   0.367237   |     -      |     -     |   1.22   \n",
            "  18    |   360   |   0.312280   |     -      |     -     |   1.22   \n",
            "  18    |   380   |   0.358026   |     -      |     -     |   1.22   \n",
            "  18    |   400   |   0.365185   |     -      |     -     |   1.22   \n",
            "  18    |   420   |   0.486431   |     -      |     -     |   1.22   \n",
            "  18    |   436   |   0.369434   |     -      |     -     |   0.97   \n",
            "----------------------------------------------------------------------\n",
            "  18    |    -    |   0.390515   |  0.446996  |   82.94   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  19    |   20    |   0.380015   |     -      |     -     |   1.27   \n",
            "  19    |   40    |   0.405117   |     -      |     -     |   1.22   \n",
            "  19    |   60    |   0.449561   |     -      |     -     |   1.22   \n",
            "  19    |   80    |   0.368635   |     -      |     -     |   1.22   \n",
            "  19    |   100   |   0.391471   |     -      |     -     |   1.22   \n",
            "  19    |   120   |   0.349436   |     -      |     -     |   1.22   \n",
            "  19    |   140   |   0.338924   |     -      |     -     |   1.22   \n",
            "  19    |   160   |   0.424780   |     -      |     -     |   1.22   \n",
            "  19    |   180   |   0.390441   |     -      |     -     |   1.22   \n",
            "  19    |   200   |   0.416433   |     -      |     -     |   1.22   \n",
            "  19    |   220   |   0.368476   |     -      |     -     |   1.22   \n",
            "  19    |   240   |   0.302958   |     -      |     -     |   1.22   \n",
            "  19    |   260   |   0.362049   |     -      |     -     |   1.22   \n",
            "  19    |   280   |   0.414675   |     -      |     -     |   1.22   \n",
            "  19    |   300   |   0.484069   |     -      |     -     |   1.22   \n",
            "  19    |   320   |   0.393774   |     -      |     -     |   1.22   \n",
            "  19    |   340   |   0.355737   |     -      |     -     |   1.22   \n",
            "  19    |   360   |   0.306425   |     -      |     -     |   1.22   \n",
            "  19    |   380   |   0.396610   |     -      |     -     |   1.22   \n",
            "  19    |   400   |   0.321326   |     -      |     -     |   1.22   \n",
            "  19    |   420   |   0.473033   |     -      |     -     |   1.22   \n",
            "  19    |   436   |   0.332269   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  19    |    -    |   0.383467   |  0.452172  |   82.74   |   30.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  20    |   20    |   0.376757   |     -      |     -     |   1.29   \n",
            "  20    |   40    |   0.401052   |     -      |     -     |   1.22   \n",
            "  20    |   60    |   0.472819   |     -      |     -     |   1.22   \n",
            "  20    |   80    |   0.385172   |     -      |     -     |   1.22   \n",
            "  20    |   100   |   0.374064   |     -      |     -     |   1.22   \n",
            "  20    |   120   |   0.348809   |     -      |     -     |   1.22   \n",
            "  20    |   140   |   0.313137   |     -      |     -     |   1.22   \n",
            "  20    |   160   |   0.420323   |     -      |     -     |   1.22   \n",
            "  20    |   180   |   0.367456   |     -      |     -     |   1.22   \n",
            "  20    |   200   |   0.393791   |     -      |     -     |   1.22   \n",
            "  20    |   220   |   0.388436   |     -      |     -     |   1.22   \n",
            "  20    |   240   |   0.300502   |     -      |     -     |   1.22   \n",
            "  20    |   260   |   0.360340   |     -      |     -     |   1.22   \n",
            "  20    |   280   |   0.384068   |     -      |     -     |   1.22   \n",
            "  20    |   300   |   0.414442   |     -      |     -     |   1.22   \n",
            "  20    |   320   |   0.372948   |     -      |     -     |   1.22   \n",
            "  20    |   340   |   0.419056   |     -      |     -     |   1.22   \n",
            "  20    |   360   |   0.310761   |     -      |     -     |   1.22   \n",
            "  20    |   380   |   0.415403   |     -      |     -     |   1.22   \n",
            "  20    |   400   |   0.334880   |     -      |     -     |   1.22   \n",
            "  20    |   420   |   0.406568   |     -      |     -     |   1.22   \n",
            "  20    |   436   |   0.334137   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  20    |    -    |   0.377434   |  0.476441  |   82.64   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  21    |   20    |   0.407935   |     -      |     -     |   1.28   \n",
            "  21    |   40    |   0.421226   |     -      |     -     |   1.22   \n",
            "  21    |   60    |   0.451831   |     -      |     -     |   1.22   \n",
            "  21    |   80    |   0.388376   |     -      |     -     |   1.22   \n",
            "  21    |   100   |   0.382430   |     -      |     -     |   1.22   \n",
            "  21    |   120   |   0.388529   |     -      |     -     |   1.22   \n",
            "  21    |   140   |   0.315091   |     -      |     -     |   1.22   \n",
            "  21    |   160   |   0.421791   |     -      |     -     |   1.22   \n",
            "  21    |   180   |   0.400593   |     -      |     -     |   1.22   \n",
            "  21    |   200   |   0.392936   |     -      |     -     |   1.22   \n",
            "  21    |   220   |   0.348058   |     -      |     -     |   1.22   \n",
            "  21    |   240   |   0.343757   |     -      |     -     |   1.22   \n",
            "  21    |   260   |   0.335252   |     -      |     -     |   1.22   \n",
            "  21    |   280   |   0.414255   |     -      |     -     |   1.22   \n",
            "  21    |   300   |   0.416485   |     -      |     -     |   1.22   \n",
            "  21    |   320   |   0.406532   |     -      |     -     |   1.22   \n",
            "  21    |   340   |   0.344344   |     -      |     -     |   1.22   \n",
            "  21    |   360   |   0.325194   |     -      |     -     |   1.22   \n",
            "  21    |   380   |   0.372474   |     -      |     -     |   1.22   \n",
            "  21    |   400   |   0.368840   |     -      |     -     |   1.22   \n",
            "  21    |   420   |   0.413904   |     -      |     -     |   1.22   \n",
            "  21    |   436   |   0.373386   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  21    |    -    |   0.383475   |  0.451048  |   83.13   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  22    |   20    |   0.394316   |     -      |     -     |   1.28   \n",
            "  22    |   40    |   0.413037   |     -      |     -     |   1.22   \n",
            "  22    |   60    |   0.472316   |     -      |     -     |   1.22   \n",
            "  22    |   80    |   0.391251   |     -      |     -     |   1.22   \n",
            "  22    |   100   |   0.355041   |     -      |     -     |   1.22   \n",
            "  22    |   120   |   0.316858   |     -      |     -     |   1.22   \n",
            "  22    |   140   |   0.335763   |     -      |     -     |   1.22   \n",
            "  22    |   160   |   0.464559   |     -      |     -     |   1.22   \n",
            "  22    |   180   |   0.413393   |     -      |     -     |   1.22   \n",
            "  22    |   200   |   0.427652   |     -      |     -     |   1.22   \n",
            "  22    |   220   |   0.381488   |     -      |     -     |   1.22   \n",
            "  22    |   240   |   0.294216   |     -      |     -     |   1.22   \n",
            "  22    |   260   |   0.363210   |     -      |     -     |   1.22   \n",
            "  22    |   280   |   0.411863   |     -      |     -     |   1.22   \n",
            "  22    |   300   |   0.424832   |     -      |     -     |   1.22   \n",
            "  22    |   320   |   0.367823   |     -      |     -     |   1.22   \n",
            "  22    |   340   |   0.369548   |     -      |     -     |   1.22   \n",
            "  22    |   360   |   0.299549   |     -      |     -     |   1.22   \n",
            "  22    |   380   |   0.368890   |     -      |     -     |   1.22   \n",
            "  22    |   400   |   0.313138   |     -      |     -     |   1.22   \n",
            "  22    |   420   |   0.419912   |     -      |     -     |   1.22   \n",
            "  22    |   436   |   0.343532   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  22    |    -    |   0.379551   |  0.465665  |   82.84   |   30.39  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  23    |   20    |   0.369606   |     -      |     -     |   1.27   \n",
            "  23    |   40    |   0.431980   |     -      |     -     |   1.22   \n",
            "  23    |   60    |   0.451145   |     -      |     -     |   1.22   \n",
            "  23    |   80    |   0.413956   |     -      |     -     |   1.22   \n",
            "  23    |   100   |   0.388413   |     -      |     -     |   1.22   \n",
            "  23    |   120   |   0.295521   |     -      |     -     |   1.22   \n",
            "  23    |   140   |   0.311840   |     -      |     -     |   1.22   \n",
            "  23    |   160   |   0.459519   |     -      |     -     |   1.23   \n",
            "  23    |   180   |   0.375824   |     -      |     -     |   1.22   \n",
            "  23    |   200   |   0.400494   |     -      |     -     |   1.22   \n",
            "  23    |   220   |   0.368697   |     -      |     -     |   1.22   \n",
            "  23    |   240   |   0.324673   |     -      |     -     |   1.22   \n",
            "  23    |   260   |   0.340919   |     -      |     -     |   1.22   \n",
            "  23    |   280   |   0.451312   |     -      |     -     |   1.22   \n",
            "  23    |   300   |   0.399165   |     -      |     -     |   1.22   \n",
            "  23    |   320   |   0.385432   |     -      |     -     |   1.22   \n",
            "  23    |   340   |   0.377572   |     -      |     -     |   1.22   \n",
            "  23    |   360   |   0.273337   |     -      |     -     |   1.22   \n",
            "  23    |   380   |   0.376090   |     -      |     -     |   1.22   \n",
            "  23    |   400   |   0.346659   |     -      |     -     |   1.22   \n",
            "  23    |   420   |   0.457370   |     -      |     -     |   1.22   \n",
            "  23    |   436   |   0.332509   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  23    |    -    |   0.379131   |  0.456542  |   82.44   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  24    |   20    |   0.352277   |     -      |     -     |   1.29   \n",
            "  24    |   40    |   0.380189   |     -      |     -     |   1.22   \n",
            "  24    |   60    |   0.443003   |     -      |     -     |   1.22   \n",
            "  24    |   80    |   0.377005   |     -      |     -     |   1.22   \n",
            "  24    |   100   |   0.407209   |     -      |     -     |   1.22   \n",
            "  24    |   120   |   0.299316   |     -      |     -     |   1.22   \n",
            "  24    |   140   |   0.337237   |     -      |     -     |   1.22   \n",
            "  24    |   160   |   0.467900   |     -      |     -     |   1.22   \n",
            "  24    |   180   |   0.382094   |     -      |     -     |   1.22   \n",
            "  24    |   200   |   0.403748   |     -      |     -     |   1.22   \n",
            "  24    |   220   |   0.338242   |     -      |     -     |   1.22   \n",
            "  24    |   240   |   0.317125   |     -      |     -     |   1.22   \n",
            "  24    |   260   |   0.338433   |     -      |     -     |   1.22   \n",
            "  24    |   280   |   0.415357   |     -      |     -     |   1.22   \n",
            "  24    |   300   |   0.397056   |     -      |     -     |   1.22   \n",
            "  24    |   320   |   0.396726   |     -      |     -     |   1.22   \n",
            "  24    |   340   |   0.321321   |     -      |     -     |   1.22   \n",
            "  24    |   360   |   0.299498   |     -      |     -     |   1.22   \n",
            "  24    |   380   |   0.393076   |     -      |     -     |   1.22   \n",
            "  24    |   400   |   0.329540   |     -      |     -     |   1.22   \n",
            "  24    |   420   |   0.440064   |     -      |     -     |   1.22   \n",
            "  24    |   436   |   0.366955   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  24    |    -    |   0.372888   |  0.453305  |   82.34   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  25    |   20    |   0.396331   |     -      |     -     |   1.27   \n",
            "  25    |   40    |   0.427283   |     -      |     -     |   1.22   \n",
            "  25    |   60    |   0.482524   |     -      |     -     |   1.22   \n",
            "  25    |   80    |   0.365051   |     -      |     -     |   1.22   \n",
            "  25    |   100   |   0.371317   |     -      |     -     |   1.22   \n",
            "  25    |   120   |   0.337060   |     -      |     -     |   1.22   \n",
            "  25    |   140   |   0.310369   |     -      |     -     |   1.22   \n",
            "  25    |   160   |   0.441004   |     -      |     -     |   1.22   \n",
            "  25    |   180   |   0.394877   |     -      |     -     |   1.22   \n",
            "  25    |   200   |   0.411786   |     -      |     -     |   1.22   \n",
            "  25    |   220   |   0.352706   |     -      |     -     |   1.22   \n",
            "  25    |   240   |   0.327545   |     -      |     -     |   1.22   \n",
            "  25    |   260   |   0.341679   |     -      |     -     |   1.22   \n",
            "  25    |   280   |   0.422284   |     -      |     -     |   1.22   \n",
            "  25    |   300   |   0.396210   |     -      |     -     |   1.22   \n",
            "  25    |   320   |   0.401868   |     -      |     -     |   1.22   \n",
            "  25    |   340   |   0.359794   |     -      |     -     |   1.22   \n",
            "  25    |   360   |   0.285754   |     -      |     -     |   1.22   \n",
            "  25    |   380   |   0.366912   |     -      |     -     |   1.22   \n",
            "  25    |   400   |   0.308561   |     -      |     -     |   1.22   \n",
            "  25    |   420   |   0.395733   |     -      |     -     |   1.22   \n",
            "  25    |   436   |   0.336718   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  25    |    -    |   0.374638   |  0.468990  |   83.04   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  26    |   20    |   0.359538   |     -      |     -     |   1.28   \n",
            "  26    |   40    |   0.404723   |     -      |     -     |   1.22   \n",
            "  26    |   60    |   0.429997   |     -      |     -     |   1.22   \n",
            "  26    |   80    |   0.372337   |     -      |     -     |   1.22   \n",
            "  26    |   100   |   0.359941   |     -      |     -     |   1.22   \n",
            "  26    |   120   |   0.313630   |     -      |     -     |   1.22   \n",
            "  26    |   140   |   0.333333   |     -      |     -     |   1.22   \n",
            "  26    |   160   |   0.405303   |     -      |     -     |   1.22   \n",
            "  26    |   180   |   0.397900   |     -      |     -     |   1.22   \n",
            "  26    |   200   |   0.391310   |     -      |     -     |   1.22   \n",
            "  26    |   220   |   0.373078   |     -      |     -     |   1.22   \n",
            "  26    |   240   |   0.333332   |     -      |     -     |   1.22   \n",
            "  26    |   260   |   0.347576   |     -      |     -     |   1.22   \n",
            "  26    |   280   |   0.429856   |     -      |     -     |   1.22   \n",
            "  26    |   300   |   0.371046   |     -      |     -     |   1.22   \n",
            "  26    |   320   |   0.382320   |     -      |     -     |   1.22   \n",
            "  26    |   340   |   0.372498   |     -      |     -     |   1.22   \n",
            "  26    |   360   |   0.296042   |     -      |     -     |   1.22   \n",
            "  26    |   380   |   0.342468   |     -      |     -     |   1.22   \n",
            "  26    |   400   |   0.340907   |     -      |     -     |   1.22   \n",
            "  26    |   420   |   0.448656   |     -      |     -     |   1.22   \n",
            "  26    |   436   |   0.338315   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  26    |    -    |   0.370454   |  0.468502  |   83.13   |   30.39  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  27    |   20    |   0.377318   |     -      |     -     |   1.28   \n",
            "  27    |   40    |   0.373606   |     -      |     -     |   1.22   \n",
            "  27    |   60    |   0.503037   |     -      |     -     |   1.22   \n",
            "  27    |   80    |   0.370674   |     -      |     -     |   1.22   \n",
            "  27    |   100   |   0.350748   |     -      |     -     |   1.22   \n",
            "  27    |   120   |   0.327639   |     -      |     -     |   1.22   \n",
            "  27    |   140   |   0.314723   |     -      |     -     |   1.22   \n",
            "  27    |   160   |   0.420882   |     -      |     -     |   1.22   \n",
            "  27    |   180   |   0.380085   |     -      |     -     |   1.22   \n",
            "  27    |   200   |   0.385304   |     -      |     -     |   1.22   \n",
            "  27    |   220   |   0.345313   |     -      |     -     |   1.22   \n",
            "  27    |   240   |   0.344469   |     -      |     -     |   1.22   \n",
            "  27    |   260   |   0.345276   |     -      |     -     |   1.22   \n",
            "  27    |   280   |   0.382883   |     -      |     -     |   1.22   \n",
            "  27    |   300   |   0.407207   |     -      |     -     |   1.22   \n",
            "  27    |   320   |   0.386070   |     -      |     -     |   1.22   \n",
            "  27    |   340   |   0.346758   |     -      |     -     |   1.22   \n",
            "  27    |   360   |   0.280960   |     -      |     -     |   1.22   \n",
            "  27    |   380   |   0.340582   |     -      |     -     |   1.22   \n",
            "  27    |   400   |   0.335370   |     -      |     -     |   1.22   \n",
            "  27    |   420   |   0.410301   |     -      |     -     |   1.22   \n",
            "  27    |   436   |   0.328117   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  27    |    -    |   0.366616   |  0.465356  |   82.54   |   30.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  28    |   20    |   0.350498   |     -      |     -     |   1.29   \n",
            "  28    |   40    |   0.410134   |     -      |     -     |   1.22   \n",
            "  28    |   60    |   0.443127   |     -      |     -     |   1.22   \n",
            "  28    |   80    |   0.402581   |     -      |     -     |   1.22   \n",
            "  28    |   100   |   0.364253   |     -      |     -     |   1.22   \n",
            "  28    |   120   |   0.324977   |     -      |     -     |   1.22   \n",
            "  28    |   140   |   0.301263   |     -      |     -     |   1.22   \n",
            "  28    |   160   |   0.421517   |     -      |     -     |   1.22   \n",
            "  28    |   180   |   0.387419   |     -      |     -     |   1.22   \n",
            "  28    |   200   |   0.360340   |     -      |     -     |   1.22   \n",
            "  28    |   220   |   0.367180   |     -      |     -     |   1.22   \n",
            "  28    |   240   |   0.308979   |     -      |     -     |   1.22   \n",
            "  28    |   260   |   0.341919   |     -      |     -     |   1.22   \n",
            "  28    |   280   |   0.388307   |     -      |     -     |   1.22   \n",
            "  28    |   300   |   0.395015   |     -      |     -     |   1.22   \n",
            "  28    |   320   |   0.358471   |     -      |     -     |   1.22   \n",
            "  28    |   340   |   0.364180   |     -      |     -     |   1.22   \n",
            "  28    |   360   |   0.296404   |     -      |     -     |   1.22   \n",
            "  28    |   380   |   0.418057   |     -      |     -     |   1.22   \n",
            "  28    |   400   |   0.306602   |     -      |     -     |   1.22   \n",
            "  28    |   420   |   0.392856   |     -      |     -     |   1.22   \n",
            "  28    |   436   |   0.316618   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  28    |    -    |   0.364984   |  0.479503  |   83.04   |   30.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  29    |   20    |   0.375425   |     -      |     -     |   1.28   \n",
            "  29    |   40    |   0.350182   |     -      |     -     |   1.22   \n",
            "  29    |   60    |   0.423279   |     -      |     -     |   1.22   \n",
            "  29    |   80    |   0.400355   |     -      |     -     |   1.22   \n",
            "  29    |   100   |   0.375570   |     -      |     -     |   1.22   \n",
            "  29    |   120   |   0.342316   |     -      |     -     |   1.22   \n",
            "  29    |   140   |   0.298403   |     -      |     -     |   1.22   \n",
            "  29    |   160   |   0.405767   |     -      |     -     |   1.22   \n",
            "  29    |   180   |   0.379822   |     -      |     -     |   1.22   \n",
            "  29    |   200   |   0.392394   |     -      |     -     |   1.22   \n",
            "  29    |   220   |   0.356676   |     -      |     -     |   1.22   \n",
            "  29    |   240   |   0.321128   |     -      |     -     |   1.22   \n",
            "  29    |   260   |   0.343255   |     -      |     -     |   1.22   \n",
            "  29    |   280   |   0.395240   |     -      |     -     |   1.22   \n",
            "  29    |   300   |   0.414416   |     -      |     -     |   1.22   \n",
            "  29    |   320   |   0.395675   |     -      |     -     |   1.22   \n",
            "  29    |   340   |   0.372503   |     -      |     -     |   1.22   \n",
            "  29    |   360   |   0.280591   |     -      |     -     |   1.22   \n",
            "  29    |   380   |   0.375011   |     -      |     -     |   1.22   \n",
            "  29    |   400   |   0.318096   |     -      |     -     |   1.22   \n",
            "  29    |   420   |   0.443494   |     -      |     -     |   1.22   \n",
            "  29    |   436   |   0.361786   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  29    |    -    |   0.369236   |  0.460216  |   82.64   |   30.39  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  30    |   20    |   0.370543   |     -      |     -     |   1.29   \n",
            "  30    |   40    |   0.365468   |     -      |     -     |   1.22   \n",
            "  30    |   60    |   0.471411   |     -      |     -     |   1.22   \n",
            "  30    |   80    |   0.372545   |     -      |     -     |   1.22   \n",
            "  30    |   100   |   0.375513   |     -      |     -     |   1.22   \n",
            "  30    |   120   |   0.329112   |     -      |     -     |   1.22   \n",
            "  30    |   140   |   0.297352   |     -      |     -     |   1.22   \n",
            "  30    |   160   |   0.390059   |     -      |     -     |   1.22   \n",
            "  30    |   180   |   0.344618   |     -      |     -     |   1.22   \n",
            "  30    |   200   |   0.395999   |     -      |     -     |   1.22   \n",
            "  30    |   220   |   0.319356   |     -      |     -     |   1.22   \n",
            "  30    |   240   |   0.323001   |     -      |     -     |   1.22   \n",
            "  30    |   260   |   0.337224   |     -      |     -     |   1.22   \n",
            "  30    |   280   |   0.457473   |     -      |     -     |   1.22   \n",
            "  30    |   300   |   0.365194   |     -      |     -     |   1.22   \n",
            "  30    |   320   |   0.396952   |     -      |     -     |   1.22   \n",
            "  30    |   340   |   0.404987   |     -      |     -     |   1.22   \n",
            "  30    |   360   |   0.277432   |     -      |     -     |   1.22   \n",
            "  30    |   380   |   0.397132   |     -      |     -     |   1.22   \n",
            "  30    |   400   |   0.344626   |     -      |     -     |   1.22   \n",
            "  30    |   420   |   0.423284   |     -      |     -     |   1.22   \n",
            "  30    |   436   |   0.338133   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  30    |    -    |   0.368344   |  0.466027  |   82.84   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  31    |   20    |   0.344174   |     -      |     -     |   1.29   \n",
            "  31    |   40    |   0.389199   |     -      |     -     |   1.22   \n",
            "  31    |   60    |   0.404342   |     -      |     -     |   1.22   \n",
            "  31    |   80    |   0.371965   |     -      |     -     |   1.22   \n",
            "  31    |   100   |   0.369928   |     -      |     -     |   1.22   \n",
            "  31    |   120   |   0.316217   |     -      |     -     |   1.22   \n",
            "  31    |   140   |   0.275775   |     -      |     -     |   1.22   \n",
            "  31    |   160   |   0.436008   |     -      |     -     |   1.22   \n",
            "  31    |   180   |   0.373143   |     -      |     -     |   1.22   \n",
            "  31    |   200   |   0.362368   |     -      |     -     |   1.22   \n",
            "  31    |   220   |   0.356166   |     -      |     -     |   1.22   \n",
            "  31    |   240   |   0.293952   |     -      |     -     |   1.22   \n",
            "  31    |   260   |   0.319808   |     -      |     -     |   1.22   \n",
            "  31    |   280   |   0.421987   |     -      |     -     |   1.22   \n",
            "  31    |   300   |   0.390655   |     -      |     -     |   1.22   \n",
            "  31    |   320   |   0.403655   |     -      |     -     |   1.22   \n",
            "  31    |   340   |   0.357306   |     -      |     -     |   1.22   \n",
            "  31    |   360   |   0.297082   |     -      |     -     |   1.22   \n",
            "  31    |   380   |   0.321968   |     -      |     -     |   1.22   \n",
            "  31    |   400   |   0.308173   |     -      |     -     |   1.22   \n",
            "  31    |   420   |   0.443877   |     -      |     -     |   1.22   \n",
            "  31    |   436   |   0.338588   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  31    |    -    |   0.359077   |  0.476035  |   82.84   |   30.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  32    |   20    |   0.367260   |     -      |     -     |   1.27   \n",
            "  32    |   40    |   0.360715   |     -      |     -     |   1.22   \n",
            "  32    |   60    |   0.417277   |     -      |     -     |   1.22   \n",
            "  32    |   80    |   0.407546   |     -      |     -     |   1.22   \n",
            "  32    |   100   |   0.357095   |     -      |     -     |   1.23   \n",
            "  32    |   120   |   0.328141   |     -      |     -     |   1.22   \n",
            "  32    |   140   |   0.326557   |     -      |     -     |   1.22   \n",
            "  32    |   160   |   0.401291   |     -      |     -     |   1.22   \n",
            "  32    |   180   |   0.394280   |     -      |     -     |   1.22   \n",
            "  32    |   200   |   0.417508   |     -      |     -     |   1.22   \n",
            "  32    |   220   |   0.342170   |     -      |     -     |   1.22   \n",
            "  32    |   240   |   0.335739   |     -      |     -     |   1.22   \n",
            "  32    |   260   |   0.325192   |     -      |     -     |   1.22   \n",
            "  32    |   280   |   0.397350   |     -      |     -     |   1.22   \n",
            "  32    |   300   |   0.368432   |     -      |     -     |   1.22   \n",
            "  32    |   320   |   0.357185   |     -      |     -     |   1.22   \n",
            "  32    |   340   |   0.366279   |     -      |     -     |   1.22   \n",
            "  32    |   360   |   0.299125   |     -      |     -     |   1.22   \n",
            "  32    |   380   |   0.354903   |     -      |     -     |   1.22   \n",
            "  32    |   400   |   0.301727   |     -      |     -     |   1.22   \n",
            "  32    |   420   |   0.470293   |     -      |     -     |   1.22   \n",
            "  32    |   436   |   0.321979   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  32    |    -    |   0.364852   |  0.474168  |   82.74   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  33    |   20    |   0.390168   |     -      |     -     |   1.29   \n",
            "  33    |   40    |   0.423256   |     -      |     -     |   1.22   \n",
            "  33    |   60    |   0.434104   |     -      |     -     |   1.22   \n",
            "  33    |   80    |   0.366181   |     -      |     -     |   1.22   \n",
            "  33    |   100   |   0.377474   |     -      |     -     |   1.22   \n",
            "  33    |   120   |   0.321127   |     -      |     -     |   1.22   \n",
            "  33    |   140   |   0.325707   |     -      |     -     |   1.22   \n",
            "  33    |   160   |   0.400291   |     -      |     -     |   1.22   \n",
            "  33    |   180   |   0.379324   |     -      |     -     |   1.22   \n",
            "  33    |   200   |   0.383130   |     -      |     -     |   1.22   \n",
            "  33    |   220   |   0.349445   |     -      |     -     |   1.22   \n",
            "  33    |   240   |   0.312561   |     -      |     -     |   1.22   \n",
            "  33    |   260   |   0.356055   |     -      |     -     |   1.22   \n",
            "  33    |   280   |   0.388304   |     -      |     -     |   1.22   \n",
            "  33    |   300   |   0.367092   |     -      |     -     |   1.22   \n",
            "  33    |   320   |   0.373308   |     -      |     -     |   1.22   \n",
            "  33    |   340   |   0.326103   |     -      |     -     |   1.22   \n",
            "  33    |   360   |   0.258042   |     -      |     -     |   1.22   \n",
            "  33    |   380   |   0.333537   |     -      |     -     |   1.22   \n",
            "  33    |   400   |   0.298127   |     -      |     -     |   1.22   \n",
            "  33    |   420   |   0.435951   |     -      |     -     |   1.22   \n",
            "  33    |   436   |   0.314415   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  33    |    -    |   0.360198   |  0.502011  |   82.74   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  34    |   20    |   0.329512   |     -      |     -     |   1.28   \n",
            "  34    |   40    |   0.382664   |     -      |     -     |   1.22   \n",
            "  34    |   60    |   0.395602   |     -      |     -     |   1.22   \n",
            "  34    |   80    |   0.368327   |     -      |     -     |   1.22   \n",
            "  34    |   100   |   0.338482   |     -      |     -     |   1.22   \n",
            "  34    |   120   |   0.271765   |     -      |     -     |   1.22   \n",
            "  34    |   140   |   0.328906   |     -      |     -     |   1.22   \n",
            "  34    |   160   |   0.408968   |     -      |     -     |   1.22   \n",
            "  34    |   180   |   0.398619   |     -      |     -     |   1.22   \n",
            "  34    |   200   |   0.394812   |     -      |     -     |   1.22   \n",
            "  34    |   220   |   0.376230   |     -      |     -     |   1.22   \n",
            "  34    |   240   |   0.275011   |     -      |     -     |   1.22   \n",
            "  34    |   260   |   0.336772   |     -      |     -     |   1.22   \n",
            "  34    |   280   |   0.463278   |     -      |     -     |   1.22   \n",
            "  34    |   300   |   0.374355   |     -      |     -     |   1.22   \n",
            "  34    |   320   |   0.370614   |     -      |     -     |   1.22   \n",
            "  34    |   340   |   0.356740   |     -      |     -     |   1.22   \n",
            "  34    |   360   |   0.281099   |     -      |     -     |   1.22   \n",
            "  34    |   380   |   0.316517   |     -      |     -     |   1.22   \n",
            "  34    |   400   |   0.315686   |     -      |     -     |   1.22   \n",
            "  34    |   420   |   0.428740   |     -      |     -     |   1.22   \n",
            "  34    |   436   |   0.345923   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  34    |    -    |   0.357250   |  0.474960  |   83.04   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  35    |   20    |   0.333845   |     -      |     -     |   1.28   \n",
            "  35    |   40    |   0.356666   |     -      |     -     |   1.22   \n",
            "  35    |   60    |   0.469630   |     -      |     -     |   1.22   \n",
            "  35    |   80    |   0.331384   |     -      |     -     |   1.22   \n",
            "  35    |   100   |   0.353551   |     -      |     -     |   1.22   \n",
            "  35    |   120   |   0.293364   |     -      |     -     |   1.22   \n",
            "  35    |   140   |   0.299730   |     -      |     -     |   1.22   \n",
            "  35    |   160   |   0.438472   |     -      |     -     |   1.22   \n",
            "  35    |   180   |   0.373284   |     -      |     -     |   1.22   \n",
            "  35    |   200   |   0.393853   |     -      |     -     |   1.22   \n",
            "  35    |   220   |   0.338577   |     -      |     -     |   1.22   \n",
            "  35    |   240   |   0.302984   |     -      |     -     |   1.22   \n",
            "  35    |   260   |   0.326379   |     -      |     -     |   1.22   \n",
            "  35    |   280   |   0.386926   |     -      |     -     |   1.22   \n",
            "  35    |   300   |   0.401703   |     -      |     -     |   1.22   \n",
            "  35    |   320   |   0.375150   |     -      |     -     |   1.22   \n",
            "  35    |   340   |   0.359333   |     -      |     -     |   1.22   \n",
            "  35    |   360   |   0.233109   |     -      |     -     |   1.22   \n",
            "  35    |   380   |   0.356699   |     -      |     -     |   1.22   \n",
            "  35    |   400   |   0.322978   |     -      |     -     |   1.22   \n",
            "  35    |   420   |   0.451433   |     -      |     -     |   1.22   \n",
            "  35    |   436   |   0.361146   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  35    |    -    |   0.357193   |  0.480367  |   82.14   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  36    |   20    |   0.359285   |     -      |     -     |   1.29   \n",
            "  36    |   40    |   0.348139   |     -      |     -     |   1.22   \n",
            "  36    |   60    |   0.424333   |     -      |     -     |   1.22   \n",
            "  36    |   80    |   0.355839   |     -      |     -     |   1.22   \n",
            "  36    |   100   |   0.402632   |     -      |     -     |   1.22   \n",
            "  36    |   120   |   0.307141   |     -      |     -     |   1.22   \n",
            "  36    |   140   |   0.304074   |     -      |     -     |   1.22   \n",
            "  36    |   160   |   0.403394   |     -      |     -     |   1.22   \n",
            "  36    |   180   |   0.369478   |     -      |     -     |   1.22   \n",
            "  36    |   200   |   0.356441   |     -      |     -     |   1.22   \n",
            "  36    |   220   |   0.321897   |     -      |     -     |   1.22   \n",
            "  36    |   240   |   0.270986   |     -      |     -     |   1.22   \n",
            "  36    |   260   |   0.368921   |     -      |     -     |   1.22   \n",
            "  36    |   280   |   0.402650   |     -      |     -     |   1.22   \n",
            "  36    |   300   |   0.371260   |     -      |     -     |   1.22   \n",
            "  36    |   320   |   0.362037   |     -      |     -     |   1.22   \n",
            "  36    |   340   |   0.351519   |     -      |     -     |   1.22   \n",
            "  36    |   360   |   0.247189   |     -      |     -     |   1.22   \n",
            "  36    |   380   |   0.379991   |     -      |     -     |   1.22   \n",
            "  36    |   400   |   0.308875   |     -      |     -     |   1.22   \n",
            "  36    |   420   |   0.452000   |     -      |     -     |   1.22   \n",
            "  36    |   436   |   0.299471   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  36    |    -    |   0.353575   |  0.482192  |   82.84   |   30.46  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  37    |   20    |   0.357668   |     -      |     -     |   1.28   \n",
            "  37    |   40    |   0.351966   |     -      |     -     |   1.22   \n",
            "  37    |   60    |   0.408843   |     -      |     -     |   1.22   \n",
            "  37    |   80    |   0.333609   |     -      |     -     |   1.22   \n",
            "  37    |   100   |   0.347511   |     -      |     -     |   1.22   \n",
            "  37    |   120   |   0.308225   |     -      |     -     |   1.22   \n",
            "  37    |   140   |   0.286914   |     -      |     -     |   1.22   \n",
            "  37    |   160   |   0.415348   |     -      |     -     |   1.22   \n",
            "  37    |   180   |   0.354782   |     -      |     -     |   1.22   \n",
            "  37    |   200   |   0.393554   |     -      |     -     |   1.22   \n",
            "  37    |   220   |   0.388200   |     -      |     -     |   1.22   \n",
            "  37    |   240   |   0.285257   |     -      |     -     |   1.22   \n",
            "  37    |   260   |   0.326160   |     -      |     -     |   1.22   \n",
            "  37    |   280   |   0.443374   |     -      |     -     |   1.22   \n",
            "  37    |   300   |   0.382893   |     -      |     -     |   1.22   \n",
            "  37    |   320   |   0.330364   |     -      |     -     |   1.22   \n",
            "  37    |   340   |   0.349534   |     -      |     -     |   1.22   \n",
            "  37    |   360   |   0.272869   |     -      |     -     |   1.22   \n",
            "  37    |   380   |   0.344289   |     -      |     -     |   1.22   \n",
            "  37    |   400   |   0.331020   |     -      |     -     |   1.22   \n",
            "  37    |   420   |   0.402999   |     -      |     -     |   1.22   \n",
            "  37    |   436   |   0.330395   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  37    |    -    |   0.352292   |  0.488942  |   82.24   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  38    |   20    |   0.363526   |     -      |     -     |   1.28   \n",
            "  38    |   40    |   0.330262   |     -      |     -     |   1.22   \n",
            "  38    |   60    |   0.457769   |     -      |     -     |   1.22   \n",
            "  38    |   80    |   0.369215   |     -      |     -     |   1.22   \n",
            "  38    |   100   |   0.396059   |     -      |     -     |   1.22   \n",
            "  38    |   120   |   0.296838   |     -      |     -     |   1.22   \n",
            "  38    |   140   |   0.267412   |     -      |     -     |   1.22   \n",
            "  38    |   160   |   0.414745   |     -      |     -     |   1.22   \n",
            "  38    |   180   |   0.346640   |     -      |     -     |   1.22   \n",
            "  38    |   200   |   0.390971   |     -      |     -     |   1.22   \n",
            "  38    |   220   |   0.285962   |     -      |     -     |   1.22   \n",
            "  38    |   240   |   0.303900   |     -      |     -     |   1.22   \n",
            "  38    |   260   |   0.320596   |     -      |     -     |   1.22   \n",
            "  38    |   280   |   0.391456   |     -      |     -     |   1.22   \n",
            "  38    |   300   |   0.405005   |     -      |     -     |   1.22   \n",
            "  38    |   320   |   0.395000   |     -      |     -     |   1.22   \n",
            "  38    |   340   |   0.355647   |     -      |     -     |   1.22   \n",
            "  38    |   360   |   0.274372   |     -      |     -     |   1.22   \n",
            "  38    |   380   |   0.304702   |     -      |     -     |   1.22   \n",
            "  38    |   400   |   0.333177   |     -      |     -     |   1.22   \n",
            "  38    |   420   |   0.381194   |     -      |     -     |   1.22   \n",
            "  38    |   436   |   0.286170   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  38    |    -    |   0.349271   |  0.499018  |   82.24   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  39    |   20    |   0.349870   |     -      |     -     |   1.29   \n",
            "  39    |   40    |   0.357678   |     -      |     -     |   1.22   \n",
            "  39    |   60    |   0.392595   |     -      |     -     |   1.22   \n",
            "  39    |   80    |   0.310347   |     -      |     -     |   1.22   \n",
            "  39    |   100   |   0.348736   |     -      |     -     |   1.22   \n",
            "  39    |   120   |   0.341549   |     -      |     -     |   1.22   \n",
            "  39    |   140   |   0.290879   |     -      |     -     |   1.22   \n",
            "  39    |   160   |   0.429372   |     -      |     -     |   1.22   \n",
            "  39    |   180   |   0.363246   |     -      |     -     |   1.22   \n",
            "  39    |   200   |   0.387352   |     -      |     -     |   1.22   \n",
            "  39    |   220   |   0.332234   |     -      |     -     |   1.22   \n",
            "  39    |   240   |   0.271719   |     -      |     -     |   1.22   \n",
            "  39    |   260   |   0.335256   |     -      |     -     |   1.22   \n",
            "  39    |   280   |   0.424789   |     -      |     -     |   1.22   \n",
            "  39    |   300   |   0.358381   |     -      |     -     |   1.22   \n",
            "  39    |   320   |   0.325473   |     -      |     -     |   1.22   \n",
            "  39    |   340   |   0.369210   |     -      |     -     |   1.22   \n",
            "  39    |   360   |   0.277220   |     -      |     -     |   1.22   \n",
            "  39    |   380   |   0.346297   |     -      |     -     |   1.22   \n",
            "  39    |   400   |   0.327666   |     -      |     -     |   1.22   \n",
            "  39    |   420   |   0.437136   |     -      |     -     |   1.22   \n",
            "  39    |   436   |   0.321630   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  39    |    -    |   0.350197   |  0.500330  |   82.24   |   30.44  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  40    |   20    |   0.365012   |     -      |     -     |   1.28   \n",
            "  40    |   40    |   0.363561   |     -      |     -     |   1.22   \n",
            "  40    |   60    |   0.395517   |     -      |     -     |   1.22   \n",
            "  40    |   80    |   0.358705   |     -      |     -     |   1.22   \n",
            "  40    |   100   |   0.327101   |     -      |     -     |   1.22   \n",
            "  40    |   120   |   0.295600   |     -      |     -     |   1.22   \n",
            "  40    |   140   |   0.299642   |     -      |     -     |   1.22   \n",
            "  40    |   160   |   0.426001   |     -      |     -     |   1.22   \n",
            "  40    |   180   |   0.388527   |     -      |     -     |   1.22   \n",
            "  40    |   200   |   0.441136   |     -      |     -     |   1.22   \n",
            "  40    |   220   |   0.291789   |     -      |     -     |   1.22   \n",
            "  40    |   240   |   0.301382   |     -      |     -     |   1.22   \n",
            "  40    |   260   |   0.305495   |     -      |     -     |   1.22   \n",
            "  40    |   280   |   0.370629   |     -      |     -     |   1.22   \n",
            "  40    |   300   |   0.391375   |     -      |     -     |   1.22   \n",
            "  40    |   320   |   0.342123   |     -      |     -     |   1.22   \n",
            "  40    |   340   |   0.317870   |     -      |     -     |   1.22   \n",
            "  40    |   360   |   0.242526   |     -      |     -     |   1.22   \n",
            "  40    |   380   |   0.332891   |     -      |     -     |   1.22   \n",
            "  40    |   400   |   0.299146   |     -      |     -     |   1.22   \n",
            "  40    |   420   |   0.427418   |     -      |     -     |   1.22   \n",
            "  40    |   436   |   0.316793   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  40    |    -    |   0.345773   |  0.507266  |   82.44   |   30.40  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  41    |   20    |   0.335053   |     -      |     -     |   1.29   \n",
            "  41    |   40    |   0.360743   |     -      |     -     |   1.22   \n",
            "  41    |   60    |   0.476845   |     -      |     -     |   1.22   \n",
            "  41    |   80    |   0.348031   |     -      |     -     |   1.22   \n",
            "  41    |   100   |   0.326306   |     -      |     -     |   1.22   \n",
            "  41    |   120   |   0.308604   |     -      |     -     |   1.22   \n",
            "  41    |   140   |   0.278343   |     -      |     -     |   1.22   \n",
            "  41    |   160   |   0.405772   |     -      |     -     |   1.22   \n",
            "  41    |   180   |   0.389058   |     -      |     -     |   1.22   \n",
            "  41    |   200   |   0.388515   |     -      |     -     |   1.22   \n",
            "  41    |   220   |   0.333776   |     -      |     -     |   1.22   \n",
            "  41    |   240   |   0.282690   |     -      |     -     |   1.22   \n",
            "  41    |   260   |   0.287712   |     -      |     -     |   1.22   \n",
            "  41    |   280   |   0.375937   |     -      |     -     |   1.22   \n",
            "  41    |   300   |   0.336351   |     -      |     -     |   1.22   \n",
            "  41    |   320   |   0.354624   |     -      |     -     |   1.22   \n",
            "  41    |   340   |   0.311305   |     -      |     -     |   1.22   \n",
            "  41    |   360   |   0.239359   |     -      |     -     |   1.22   \n",
            "  41    |   380   |   0.348173   |     -      |     -     |   1.22   \n",
            "  41    |   400   |   0.302843   |     -      |     -     |   1.22   \n",
            "  41    |   420   |   0.424486   |     -      |     -     |   1.22   \n",
            "  41    |   436   |   0.324316   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  41    |    -    |   0.342825   |  0.495778  |   82.44   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  42    |   20    |   0.361843   |     -      |     -     |   1.29   \n",
            "  42    |   40    |   0.375634   |     -      |     -     |   1.22   \n",
            "  42    |   60    |   0.417215   |     -      |     -     |   1.22   \n",
            "  42    |   80    |   0.346664   |     -      |     -     |   1.22   \n",
            "  42    |   100   |   0.352289   |     -      |     -     |   1.22   \n",
            "  42    |   120   |   0.313836   |     -      |     -     |   1.22   \n",
            "  42    |   140   |   0.300888   |     -      |     -     |   1.22   \n",
            "  42    |   160   |   0.438145   |     -      |     -     |   1.22   \n",
            "  42    |   180   |   0.357885   |     -      |     -     |   1.22   \n",
            "  42    |   200   |   0.388242   |     -      |     -     |   1.22   \n",
            "  42    |   220   |   0.350770   |     -      |     -     |   1.22   \n",
            "  42    |   240   |   0.293912   |     -      |     -     |   1.22   \n",
            "  42    |   260   |   0.332420   |     -      |     -     |   1.22   \n",
            "  42    |   280   |   0.379382   |     -      |     -     |   1.22   \n",
            "  42    |   300   |   0.375933   |     -      |     -     |   1.22   \n",
            "  42    |   320   |   0.369068   |     -      |     -     |   1.22   \n",
            "  42    |   340   |   0.338707   |     -      |     -     |   1.22   \n",
            "  42    |   360   |   0.247193   |     -      |     -     |   1.22   \n",
            "  42    |   380   |   0.329512   |     -      |     -     |   1.22   \n",
            "  42    |   400   |   0.314321   |     -      |     -     |   1.22   \n",
            "  42    |   420   |   0.413342   |     -      |     -     |   1.22   \n",
            "  42    |   436   |   0.317145   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  42    |    -    |   0.350984   |  0.500443  |   82.24   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  43    |   20    |   0.367655   |     -      |     -     |   1.28   \n",
            "  43    |   40    |   0.361016   |     -      |     -     |   1.22   \n",
            "  43    |   60    |   0.411024   |     -      |     -     |   1.22   \n",
            "  43    |   80    |   0.327475   |     -      |     -     |   1.22   \n",
            "  43    |   100   |   0.355413   |     -      |     -     |   1.22   \n",
            "  43    |   120   |   0.264144   |     -      |     -     |   1.22   \n",
            "  43    |   140   |   0.281856   |     -      |     -     |   1.22   \n",
            "  43    |   160   |   0.385054   |     -      |     -     |   1.22   \n",
            "  43    |   180   |   0.367841   |     -      |     -     |   1.22   \n",
            "  43    |   200   |   0.378958   |     -      |     -     |   1.22   \n",
            "  43    |   220   |   0.316946   |     -      |     -     |   1.22   \n",
            "  43    |   240   |   0.297158   |     -      |     -     |   1.22   \n",
            "  43    |   260   |   0.327893   |     -      |     -     |   1.22   \n",
            "  43    |   280   |   0.359874   |     -      |     -     |   1.22   \n",
            "  43    |   300   |   0.372359   |     -      |     -     |   1.22   \n",
            "  43    |   320   |   0.382982   |     -      |     -     |   1.22   \n",
            "  43    |   340   |   0.350018   |     -      |     -     |   1.22   \n",
            "  43    |   360   |   0.241317   |     -      |     -     |   1.22   \n",
            "  43    |   380   |   0.322566   |     -      |     -     |   1.22   \n",
            "  43    |   400   |   0.284181   |     -      |     -     |   1.22   \n",
            "  43    |   420   |   0.394373   |     -      |     -     |   1.22   \n",
            "  43    |   436   |   0.330932   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  43    |    -    |   0.340194   |  0.510622  |   82.24   |   30.44  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  44    |   20    |   0.311067   |     -      |     -     |   1.28   \n",
            "  44    |   40    |   0.367960   |     -      |     -     |   1.22   \n",
            "  44    |   60    |   0.393678   |     -      |     -     |   1.22   \n",
            "  44    |   80    |   0.384918   |     -      |     -     |   1.22   \n",
            "  44    |   100   |   0.345079   |     -      |     -     |   1.22   \n",
            "  44    |   120   |   0.298026   |     -      |     -     |   1.22   \n",
            "  44    |   140   |   0.283961   |     -      |     -     |   1.22   \n",
            "  44    |   160   |   0.403269   |     -      |     -     |   1.22   \n",
            "  44    |   180   |   0.351390   |     -      |     -     |   1.22   \n",
            "  44    |   200   |   0.388581   |     -      |     -     |   1.22   \n",
            "  44    |   220   |   0.331152   |     -      |     -     |   1.22   \n",
            "  44    |   240   |   0.294871   |     -      |     -     |   1.22   \n",
            "  44    |   260   |   0.314724   |     -      |     -     |   1.22   \n",
            "  44    |   280   |   0.381445   |     -      |     -     |   1.22   \n",
            "  44    |   300   |   0.381050   |     -      |     -     |   1.22   \n",
            "  44    |   320   |   0.357945   |     -      |     -     |   1.22   \n",
            "  44    |   340   |   0.320658   |     -      |     -     |   1.22   \n",
            "  44    |   360   |   0.266235   |     -      |     -     |   1.22   \n",
            "  44    |   380   |   0.355470   |     -      |     -     |   1.22   \n",
            "  44    |   400   |   0.293513   |     -      |     -     |   1.22   \n",
            "  44    |   420   |   0.414375   |     -      |     -     |   1.22   \n",
            "  44    |   436   |   0.340415   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  44    |    -    |   0.344497   |  0.499092  |   82.84   |   30.39  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  45    |   20    |   0.334017   |     -      |     -     |   1.27   \n",
            "  45    |   40    |   0.354378   |     -      |     -     |   1.22   \n",
            "  45    |   60    |   0.495357   |     -      |     -     |   1.22   \n",
            "  45    |   80    |   0.337673   |     -      |     -     |   1.22   \n",
            "  45    |   100   |   0.344281   |     -      |     -     |   1.22   \n",
            "  45    |   120   |   0.307129   |     -      |     -     |   1.22   \n",
            "  45    |   140   |   0.276123   |     -      |     -     |   1.22   \n",
            "  45    |   160   |   0.399585   |     -      |     -     |   1.22   \n",
            "  45    |   180   |   0.348644   |     -      |     -     |   1.22   \n",
            "  45    |   200   |   0.386609   |     -      |     -     |   1.22   \n",
            "  45    |   220   |   0.331700   |     -      |     -     |   1.22   \n",
            "  45    |   240   |   0.285638   |     -      |     -     |   1.22   \n",
            "  45    |   260   |   0.350835   |     -      |     -     |   1.22   \n",
            "  45    |   280   |   0.398797   |     -      |     -     |   1.22   \n",
            "  45    |   300   |   0.375264   |     -      |     -     |   1.22   \n",
            "  45    |   320   |   0.353118   |     -      |     -     |   1.22   \n",
            "  45    |   340   |   0.305798   |     -      |     -     |   1.22   \n",
            "  45    |   360   |   0.256660   |     -      |     -     |   1.22   \n",
            "  45    |   380   |   0.340347   |     -      |     -     |   1.22   \n",
            "  45    |   400   |   0.288628   |     -      |     -     |   1.22   \n",
            "  45    |   420   |   0.429085   |     -      |     -     |   1.22   \n",
            "  45    |   436   |   0.343734   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  45    |    -    |   0.347430   |  0.506813  |   82.44   |   30.42  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  46    |   20    |   0.334475   |     -      |     -     |   1.27   \n",
            "  46    |   40    |   0.332598   |     -      |     -     |   1.22   \n",
            "  46    |   60    |   0.432353   |     -      |     -     |   1.22   \n",
            "  46    |   80    |   0.361803   |     -      |     -     |   1.22   \n",
            "  46    |   100   |   0.343541   |     -      |     -     |   1.22   \n",
            "  46    |   120   |   0.287617   |     -      |     -     |   1.22   \n",
            "  46    |   140   |   0.277068   |     -      |     -     |   1.22   \n",
            "  46    |   160   |   0.386925   |     -      |     -     |   1.22   \n",
            "  46    |   180   |   0.341260   |     -      |     -     |   1.22   \n",
            "  46    |   200   |   0.378107   |     -      |     -     |   1.22   \n",
            "  46    |   220   |   0.299306   |     -      |     -     |   1.22   \n",
            "  46    |   240   |   0.289210   |     -      |     -     |   1.22   \n",
            "  46    |   260   |   0.305330   |     -      |     -     |   1.22   \n",
            "  46    |   280   |   0.372920   |     -      |     -     |   1.22   \n",
            "  46    |   300   |   0.401254   |     -      |     -     |   1.22   \n",
            "  46    |   320   |   0.344505   |     -      |     -     |   1.22   \n",
            "  46    |   340   |   0.349631   |     -      |     -     |   1.22   \n",
            "  46    |   360   |   0.267451   |     -      |     -     |   1.22   \n",
            "  46    |   380   |   0.361149   |     -      |     -     |   1.22   \n",
            "  46    |   400   |   0.296514   |     -      |     -     |   1.22   \n",
            "  46    |   420   |   0.406202   |     -      |     -     |   1.22   \n",
            "  46    |   436   |   0.312949   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  46    |    -    |   0.340334   |  0.499252  |   82.54   |   30.39  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  47    |   20    |   0.336588   |     -      |     -     |   1.27   \n",
            "  47    |   40    |   0.346728   |     -      |     -     |   1.22   \n",
            "  47    |   60    |   0.366930   |     -      |     -     |   1.22   \n",
            "  47    |   80    |   0.319079   |     -      |     -     |   1.22   \n",
            "  47    |   100   |   0.358948   |     -      |     -     |   1.22   \n",
            "  47    |   120   |   0.275965   |     -      |     -     |   1.22   \n",
            "  47    |   140   |   0.244741   |     -      |     -     |   1.22   \n",
            "  47    |   160   |   0.406351   |     -      |     -     |   1.22   \n",
            "  47    |   180   |   0.338727   |     -      |     -     |   1.22   \n",
            "  47    |   200   |   0.383871   |     -      |     -     |   1.22   \n",
            "  47    |   220   |   0.344634   |     -      |     -     |   1.22   \n",
            "  47    |   240   |   0.299602   |     -      |     -     |   1.22   \n",
            "  47    |   260   |   0.336165   |     -      |     -     |   1.22   \n",
            "  47    |   280   |   0.331191   |     -      |     -     |   1.22   \n",
            "  47    |   300   |   0.345484   |     -      |     -     |   1.22   \n",
            "  47    |   320   |   0.368856   |     -      |     -     |   1.22   \n",
            "  47    |   340   |   0.299751   |     -      |     -     |   1.23   \n",
            "  47    |   360   |   0.251600   |     -      |     -     |   1.22   \n",
            "  47    |   380   |   0.329963   |     -      |     -     |   1.22   \n",
            "  47    |   400   |   0.291356   |     -      |     -     |   1.22   \n",
            "  47    |   420   |   0.450489   |     -      |     -     |   1.22   \n",
            "  47    |   436   |   0.305352   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  47    |    -    |   0.333553   |  0.506851  |   83.33   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  48    |   20    |   0.311071   |     -      |     -     |   1.28   \n",
            "  48    |   40    |   0.330205   |     -      |     -     |   1.22   \n",
            "  48    |   60    |   0.403395   |     -      |     -     |   1.22   \n",
            "  48    |   80    |   0.372692   |     -      |     -     |   1.22   \n",
            "  48    |   100   |   0.336167   |     -      |     -     |   1.22   \n",
            "  48    |   120   |   0.301722   |     -      |     -     |   1.22   \n",
            "  48    |   140   |   0.264673   |     -      |     -     |   1.22   \n",
            "  48    |   160   |   0.391890   |     -      |     -     |   1.22   \n",
            "  48    |   180   |   0.355146   |     -      |     -     |   1.22   \n",
            "  48    |   200   |   0.347622   |     -      |     -     |   1.22   \n",
            "  48    |   220   |   0.305380   |     -      |     -     |   1.22   \n",
            "  48    |   240   |   0.286414   |     -      |     -     |   1.22   \n",
            "  48    |   260   |   0.281465   |     -      |     -     |   1.22   \n",
            "  48    |   280   |   0.341566   |     -      |     -     |   1.22   \n",
            "  48    |   300   |   0.331193   |     -      |     -     |   1.22   \n",
            "  48    |   320   |   0.342363   |     -      |     -     |   1.22   \n",
            "  48    |   340   |   0.371590   |     -      |     -     |   1.22   \n",
            "  48    |   360   |   0.236367   |     -      |     -     |   1.22   \n",
            "  48    |   380   |   0.343724   |     -      |     -     |   1.22   \n",
            "  48    |   400   |   0.310021   |     -      |     -     |   1.22   \n",
            "  48    |   420   |   0.371208   |     -      |     -     |   1.22   \n",
            "  48    |   436   |   0.279878   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  48    |    -    |   0.328390   |  0.511428  |   82.84   |   30.41  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  49    |   20    |   0.356144   |     -      |     -     |   1.29   \n",
            "  49    |   40    |   0.345320   |     -      |     -     |   1.22   \n",
            "  49    |   60    |   0.430710   |     -      |     -     |   1.22   \n",
            "  49    |   80    |   0.310015   |     -      |     -     |   1.22   \n",
            "  49    |   100   |   0.310975   |     -      |     -     |   1.22   \n",
            "  49    |   120   |   0.284476   |     -      |     -     |   1.22   \n",
            "  49    |   140   |   0.264478   |     -      |     -     |   1.22   \n",
            "  49    |   160   |   0.412481   |     -      |     -     |   1.22   \n",
            "  49    |   180   |   0.333829   |     -      |     -     |   1.22   \n",
            "  49    |   200   |   0.358599   |     -      |     -     |   1.22   \n",
            "  49    |   220   |   0.354252   |     -      |     -     |   1.22   \n",
            "  49    |   240   |   0.290699   |     -      |     -     |   1.22   \n",
            "  49    |   260   |   0.341542   |     -      |     -     |   1.22   \n",
            "  49    |   280   |   0.384469   |     -      |     -     |   1.22   \n",
            "  49    |   300   |   0.347144   |     -      |     -     |   1.22   \n",
            "  49    |   320   |   0.400032   |     -      |     -     |   1.22   \n",
            "  49    |   340   |   0.337894   |     -      |     -     |   1.22   \n",
            "  49    |   360   |   0.249621   |     -      |     -     |   1.22   \n",
            "  49    |   380   |   0.337264   |     -      |     -     |   1.22   \n",
            "  49    |   400   |   0.280225   |     -      |     -     |   1.22   \n",
            "  49    |   420   |   0.391490   |     -      |     -     |   1.22   \n",
            "  49    |   436   |   0.319319   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  49    |    -    |   0.338440   |  0.506435  |   83.23   |   30.43  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "  50    |   20    |   0.318040   |     -      |     -     |   1.28   \n",
            "  50    |   40    |   0.344440   |     -      |     -     |   1.22   \n",
            "  50    |   60    |   0.422791   |     -      |     -     |   1.23   \n",
            "  50    |   80    |   0.320168   |     -      |     -     |   1.22   \n",
            "  50    |   100   |   0.352303   |     -      |     -     |   1.22   \n",
            "  50    |   120   |   0.332382   |     -      |     -     |   1.22   \n",
            "  50    |   140   |   0.257341   |     -      |     -     |   1.22   \n",
            "  50    |   160   |   0.391184   |     -      |     -     |   1.22   \n",
            "  50    |   180   |   0.342231   |     -      |     -     |   1.22   \n",
            "  50    |   200   |   0.356141   |     -      |     -     |   1.22   \n",
            "  50    |   220   |   0.306145   |     -      |     -     |   1.22   \n",
            "  50    |   240   |   0.286557   |     -      |     -     |   1.22   \n",
            "  50    |   260   |   0.314425   |     -      |     -     |   1.22   \n",
            "  50    |   280   |   0.381527   |     -      |     -     |   1.22   \n",
            "  50    |   300   |   0.369217   |     -      |     -     |   1.22   \n",
            "  50    |   320   |   0.320289   |     -      |     -     |   1.22   \n",
            "  50    |   340   |   0.341263   |     -      |     -     |   1.22   \n",
            "  50    |   360   |   0.265677   |     -      |     -     |   1.22   \n",
            "  50    |   380   |   0.338550   |     -      |     -     |   1.22   \n",
            "  50    |   400   |   0.312299   |     -      |     -     |   1.22   \n",
            "  50    |   420   |   0.378556   |     -      |     -     |   1.22   \n",
            "  50    |   436   |   0.345516   |     -      |     -     |   0.96   \n",
            "----------------------------------------------------------------------\n",
            "  50    |    -    |   0.336103   |  0.507754  |   83.04   |   30.44  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer ,scheduler= initialize_model(epochs=50)\n",
        "train(bert_classifier,optimizer,scheduler, train_dataloader, val_dataloader, epochs=50, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "0etVinYuwlXO"
      },
      "outputs": [],
      "source": [
        "# store the model in pickle file\n",
        "import pickle\n",
        "filename = 'arabert_model.sav'\n",
        "pickle.dump(bert_classifier, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qetcpFTFzWvX"
      },
      "outputs": [],
      "source": [
        "# # Loading the model (to avoid retraining in reruns)\n",
        "\n",
        "# import pickle\n",
        "# filename = 'trained_model_mini_with_emojis.sav'\n",
        "# f = open(filename, 'rb')\n",
        "# bert_classifier = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "i6aOUOyjzb1S"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    #model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        \n",
        "        all_logits.append(logits)\n",
        "         # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    # print(all_logits.shape)\n",
        "    # print(all_logits)\n",
        "    pred_labels=[]\n",
        "    for log in all_logits:\n",
        "      #print(len(log))\n",
        "      highest_score=torch.argmax(log)\n",
        "      #print(highest_score.item())\n",
        "       \n",
        "      pred_labels.append(highest_score)\n",
        "      \n",
        "      \n",
        "    # Apply softmax to calculate probabilities\n",
        "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_val==val_labels)"
      ],
      "metadata": {
        "id": "RJCiqEzSD04X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "owCd6ZlZzsQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8aef2e2-ea8a-41e0-e487-857c452812c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91       804\n",
            "           1       0.54      0.35      0.42       126\n",
            "           2       0.52      0.33      0.40        70\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.64      0.54      0.58      1000\n",
            "weighted avg       0.81      0.83      0.81      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute predicted probabilities on the validation set\n",
        "y_pred = bert_predict(bert_classifier, val_dataloader)\n",
        "print(len(y_val))\n",
        "print(len(y_pred))\n",
        "print(classification_report(y_val, torch.tensor(y_pred)))\n",
        "# Evaluate the Bert classifier\n",
        "#evaluate_roc(probs, y_val)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "9faa27367d8809f17efa01381c296b23b33e7966403c8c5a88e68f02f779827d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}