{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDF(filepath:str):\n",
    "    '''\n",
    "    The function gets the file path of csv file, produces new file (processed.csv) after preprocessing for exploring (includes stopwords)\n",
    "    Return\n",
    "    data_x: list of tokenized sentences\n",
    "    data_y: list of labels\n",
    "    '''\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    #REMAINING TODO: Replace emojis with sentiment\n",
    "    #REMAINING TODO: Stemming and lemmatization if needed\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    #changing category labels to numbers\n",
    "    df['category'] = pd.Categorical(df['category'], categories=df['category'].unique()).codes\n",
    "\n",
    "    ### SOME TWEET CLEANING\n",
    "    df['text'].replace(to_replace =r'http[\\da-zA-Z:/.-]*\\b', value = '', regex = True, inplace=True) #Removing URLs\n",
    "    df['text'].replace(to_replace =r'<LF>', value = ' ', regex = True, inplace=True) #Remove tags\n",
    "    df['text'].replace(to_replace =r'#', value = '', regex = True, inplace=True) #Remove hashtags\n",
    "    df['text'].replace(to_replace =r'_', value = ' ', regex = True, inplace=True) #Replace underscores\n",
    "    df['text'].replace(to_replace =r'ـ', value = '', regex = True, inplace=True) #Remove reduntant word elongation عــــاجل => عاجل\n",
    "    df['text'].replace(to_replace =r'@\\w\\b', value = '', regex = True, inplace=True) #Remove mentions\n",
    "    df['text'].replace(to_replace =r'@', value = '', regex = True, inplace=True) #Remove @\n",
    "    df['text'].replace(to_replace =r'USER', value = '', regex = True, inplace=True) #Remove all occurences of USER\n",
    "    df['text'].replace(to_replace =r'\\b[أإآ]', value = 'ا', regex = True, inplace=True) #Change all forms of Alif at the beginning of word to one form (أسد) -> (اسد) #Not Sure if important\n",
    "    df['text'].replace(to_replace =r'ة\\b', value = 'ه', regex = True, inplace=True) #Assume people mix between ة and ه at end of words, we make it all ه #Not Sure if important\n",
    "    df['text'].replace(to_replace =r'[ڤڨ]', value = 'ف', regex = True, inplace=True) #Replace ڤ with ف\n",
    "\n",
    "    df['text'].replace(to_replace =r'[Cc][Oo][Vv][Ii][Dd](19)?(-19)?( 19)?( - 19)?', value = 'كورونا', regex = True, inplace=True) #Change all forms of covid words to one form (covid19, coronavirus, فيروس كورونا,...) to كورونا\n",
    "    df['text'].replace(to_replace =r'[Cc][Oo][Rr][Oo][Nn][Aa](\\s?[Vv][Ii][Rr][Uu][Ss])?', value = 'كورونا', regex = True, inplace=True) \n",
    "    df['text'].replace(to_replace =r'(فيرو?س )?(ال)?كورونا', value = 'كورونا', regex = True, inplace=True) \n",
    "    df['text'].replace(to_replace =r'كوفيد(19)?(-19)?( 19)?( - 19)?(١٩)?(-١٩)?( ١٩)?( - ١٩)?', value = 'كورونا', regex = True, inplace=True)\n",
    "\n",
    "    df['text'].replace(to_replace =r'[^\\w\\s]', value = ' ', regex = True, inplace=True) #remove punctuation\n",
    "    df['text'].replace(to_replace =r'( )+', value = ' ', regex = True, inplace=True) #remove long spaces\n",
    "    df['text'].replace(to_replace =r'[0-9٠١٢٣٤٥٦٧٨٩]', value = '', regex = True, inplace=True) #remove numbers\n",
    "\n",
    "    df['text'].to_csv('processed.csv')\n",
    "\n",
    "    ### TOKENIZATION\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "    arabicStopwords = stopwords.words('arabic')\n",
    "    \n",
    "\n",
    "    for index, item in df.iterrows():\n",
    "        tweet = item['text']\n",
    "        tokenizedTweet = []\n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in arabicStopwords):  # remove stopwords\n",
    "                tokenizedTweet.append(word)\n",
    "\n",
    "        data_x.append(tokenizedTweet)\n",
    "        data_y.append([item['category'], item['stance']])\n",
    "\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "labels = []\n",
    "tokens, labels =preprocessDF('./dataset/train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cad911e89c9cd81e6830ed7e3dceb0d7faa858acac22c42d691ca01e2d61a3cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
